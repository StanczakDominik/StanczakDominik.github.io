var tipuesearch = {
  "pages": [
    {
      "title": "Export YouTube's Watch Later playlist",
      "text": "YouTube is increasingly becoming a walled garden. One of the dark patterns I see in it is that the Watch Later playlist, which you may have gathered over what feels like a millenia (so many lectures, so little time...), cannot be exported - you can only add one video at a time to another playlist. That obviously doesn't scale. Here's a quick trick to solve that!\n\nTake this URL:\nhttps://www.youtube.com/playlist?list=WL\nAppend &disable_polymer=true to it:\nhttps://www.youtube.com/playlist?list=WL&disable_polymer=true\nAnd this should bring you to an older version of the website, where if you click the 3-dot \"More\" menu, you'll find an \"Add all to...\" button. This allows you to export your Watch Later playlist to another playlist, without the annoying limitations. Once you're there, you can use better software (NewPipe, for example, or another solution from the many available) to store the playlist elsewhere. I did not investigate those, as I simply wanted to mirror my Watch Later playlist in NewPipe.\nSadly, the same method does not work for the Liked Videos page - which is why you should probably not trust YouTube with your data as much as you likely do now.",
      "tags": "",
      "url": "https://stanczakdominik.github.io/posts/youtube-watch-later-export.rst/"
    },
    {
      "title": "Self-organized criticality - student project post-mortem",
      "text": "We just got finished with our student team project, which you can find here, and I thought I'd do a little post mortem on it.\nIt's a neat little project that implements various simulations of self organized criticality on 2D grids. What is self organized criticality, you might ask? Dunno, I can't tell you.\nAll right, I do know a little. Imagine the ising system I've written about before. In the version without an external magnetic field, it has a single important parameter that we can set - the temperature. If you sweep through the values of temperature, you can find a single point where the behavior of the system changes qualitatively - order wins over disorder at temperatures below roughly 2.72 in reasonable [set everything to 1] units. Near that value - at criticality - you get large scale behavior, huge fluctuations, exponential slowdowns, etc.\nSelf organized criticality, as I currently understand it, is basically that, except that as you run your simulation, you realize that it displays criticality for a wide range of parameters (here - temperature). To quote Wikipedia (emphasis mine):\nthe complexity observed emerged in a robust manner that did not depend on finely tuned details of the system: variable parameters in the model could be changed widely without affecting the emergence of critical behavior: hence, self-organized criticality.\n\nHere's an example, a simulation of forest fire (yellow being fire, green being trees, and purple being ash, from which trees can regrow):\n\n\n\n\n\n\nIn\u00a0[30]:\n\n    \nimport SOC\nmodel = SOC.Forest(L=100, f=0.0001)  # f being probability of lightning strike\nmodel.run(500, wait_for_n_iters=500)\nmodel.animate_states(notebook=True)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nWaiting for wait_for_n_iters=500 iterations before collecting data. This should let the system thermalize.\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#4df1ffee-10ee-4382-afce-5362641f0885');\n\n\n{\"model_id\": \"3f4ec7240b464c93b12801deb235f287\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we did look for from a practical standpoint in our simulations was power law scaling of the number of iterations for each avalanche in a system, of avalanche size... and we did find that!\nHere's another model we implemented, called the Manna model:\n\n\n\n\n\n\nIn\u00a0[19]:\n\n    \nmanna = SOC.Manna(L=30)\nmanna.run(500, wait_for_n_iters = 500)\nmanna.animate_states(notebook=True)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nWaiting for wait_for_n_iters=500 iterations before collecting data. This should let the system thermalize.\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#dc6c3a7f-597c-41b0-95e3-4f25d8e8ac4c');\n\n\n{\"model_id\": \"9dff614886be46db8c9c9b4371a39611\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\n\n\n\n\n\n\n\nThis deserves a bit of a longer runtime:\n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \nmanna_long = SOC.Manna(L=30, save_every = 100)\nmanna_long.run(50000, wait_for_n_iters = 20000)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nWaiting for wait_for_n_iters=20000 iterations before collecting data. This should let the system thermalize.\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#f90fc85f-ad7d-4e82-a51d-176ff5121bcd');\n\n\n{\"model_id\": \"9a40b297eb504c29af28725f28dc7ea5\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[26]:\n\n    \nmanna_long.get_exponent(low = 10, high=100)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\ny = 23317.107 exp(-1.3904 x)\n\n\n\n\n\n\n    Out[26]:\n\n\n\n\n\n{'exponent': -1.3903903386497791, 'intercept': 4.367674673827714}\n\n\n\n\n\n\n\n\n\n\n\nYou can find the project on GitHub here.\nApparently, SOC has applications basically everywhere, but I haven't dug into those yet. There's are two articles on my to-read list that relate SOC to ELMs in fusion reactors and turbulent transport. I might do writeups on those in a later post.\n\n\n\n\n \n\n\n{\"state\": {\"006b18c4c02342dfba52f998a3088784\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_55e29d51dcbf49e4b3caffd03d945c6d\", \"IPY_MODEL_42a7aaa693d74bef89f1ced1403c62cc\"], \"layout\": \"IPY_MODEL_106ee5af82714c3fabbbb28ed368bf89\"}}, \"0138097c811249b19696b584d781b183\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"047bfc9894ff4f7ead3d90593781153d\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"0569bc1e611a43378476986a15f5a9e9\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"06c15915e7064e86b36cc7274f62652d\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"074284f3605e4889a7625906e8763bc9\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"success\", \"description\": \"100%\", \"layout\": \"IPY_MODEL_bd0d13cfda43478da6eb446c72b8f9ff\", \"max\": 3000, \"style\": \"IPY_MODEL_1356cf834bc14a49b10e76cb0547d75c\", \"value\": 3000}}, \"075b2abaa458428ab925795dd5fbbb33\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_8973176204974eda88cc6602f6885adc\", \"IPY_MODEL_abe49ced1add428eb5ac5aafc08c58e3\"], \"layout\": \"IPY_MODEL_498901bec88540ed851b9c33c62b4917\"}}, \"07af14b253a844a3ad063523f0e2d180\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_bdb3f76b77ab4c5da94476049124a9b2\", \"IPY_MODEL_7e64df1ad5774e9c8d970d1d0cc1cdb1\"], \"layout\": \"IPY_MODEL_1e62455cbf3c4274b9f14e364b8fe516\"}}, \"0b752a853f5949ecabae0def17cef1f1\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"0ce8d7f5054640839b72f4f03b87f34f\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"106ee5af82714c3fabbbb28ed368bf89\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"1356cf834bc14a49b10e76cb0547d75c\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"17a5d153712849eab6aa590553a305f1\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"success\", \"description\": \"100%\", \"layout\": \"IPY_MODEL_bce90aa97e7a451296ce5a420ce42158\", \"max\": 1000, \"style\": \"IPY_MODEL_0b752a853f5949ecabae0def17cef1f1\", \"value\": 1000}}, \"184c5100ce7c492d9bd4fc22f4fe6ebd\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"189c40cc24624b4593db2b5ea051233f\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"191fa70cd5a7456390b8efa1e2199812\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"1b2c44c72d5f44f0b7c114002f5b773c\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"1c47301615ed49f9880e367125f63d8f\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"1e62455cbf3c4274b9f14e364b8fe516\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"1e6c857f9c2744b5bfa64ea94d144143\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_074284f3605e4889a7625906e8763bc9\", \"IPY_MODEL_d7134c4df0fa46588a08f33c75a91d98\"], \"layout\": \"IPY_MODEL_3f283d13eb0b4a71bf8e95e8cf8b894b\"}}, \"1f3b883315a4452698312c0eddc0cb6e\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_747478bcf38540c0a022638b1fe4f1bb\", \"IPY_MODEL_b8911b09f9734aa8b7c80e403b5dffd2\"], \"layout\": \"IPY_MODEL_e49c91459c1b4b67a4a6981fe1ad799b\"}}, \"261cdf09822f499ab14559eb8ee0ac7a\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"299832094d474726994bf38d140a53f7\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_0138097c811249b19696b584d781b183\", \"style\": \"IPY_MODEL_ddf7b57700194116a9ace472fe2ad396\", \"value\": \" 1356/3000 [00:40&lt;00:22, 72.01it/s]\"}}, \"2a3a50badad64fc790ae6bbeefec28f2\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"success\", \"description\": \"100%\", \"layout\": \"IPY_MODEL_5fe4890ee76c40b79680414a30f69286\", \"max\": 1000, \"style\": \"IPY_MODEL_aa924e9d7b7449bdb39cd87d953cf1b9\", \"value\": 1000}}, \"2de7da3d53264768972844147be16644\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"2f095a594c614661b537260781fb6690\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"32b97edc054042818237c17259222c6e\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"34203a88540742df89a1949a388ad86b\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"34594a065c6947a6a6140f8e28658bbb\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_aced2c4149ac4666b843b7ea60695d9f\", \"style\": \"IPY_MODEL_32b97edc054042818237c17259222c6e\", \"value\": \" 7000/7000 [21:59&lt;00:00,  5.30it/s]\"}}, \"377c0371d7bf441ba0f78627af6802bb\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"379aa2cf88e5419fab61ae62e4cba5ae\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_db4667078171470daf14664eabe46a9d\", \"IPY_MODEL_ca854c0886404a5983899da0367b2411\"], \"layout\": \"IPY_MODEL_047bfc9894ff4f7ead3d90593781153d\"}}, \"382b3160cd184b88b2b1e7bc21fe01f4\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"danger\", \"description\": \" 50%\", \"layout\": \"IPY_MODEL_f38de1c1fc244361b4da1c06e2bb6820\", \"max\": 7000, \"style\": \"IPY_MODEL_c36ec375acd04449bc03ef3511f7411a\", \"value\": 3487}}, \"39465a4df1804691bbcd05be4302e540\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"success\", \"description\": \"100%\", \"layout\": \"IPY_MODEL_4b289665ef7d48f4b5d47c2ac89351b0\", \"max\": 7000, \"style\": \"IPY_MODEL_41c19320636e402792842481ebf9f7de\", \"value\": 7000}}, \"3b5de5be432a4ba881fe1619d394735b\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_2a3a50badad64fc790ae6bbeefec28f2\", \"IPY_MODEL_c12389b37057489398bf585709aa47f0\"], \"layout\": \"IPY_MODEL_189c40cc24624b4593db2b5ea051233f\"}}, \"3f283d13eb0b4a71bf8e95e8cf8b894b\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"3f4ec7240b464c93b12801deb235f287\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_cc239cd7b8474442a54dfa08ab834d55\", \"IPY_MODEL_a6183030748c4d1dbb9b84f38d07c18e\"], \"layout\": \"IPY_MODEL_1c47301615ed49f9880e367125f63d8f\"}}, \"41c19320636e402792842481ebf9f7de\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"42a7aaa693d74bef89f1ced1403c62cc\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_0ce8d7f5054640839b72f4f03b87f34f\", \"style\": \"IPY_MODEL_f7b64aeaa7254a5db767db15d3bfea16\", \"value\": \" 799/7000 [00:21&lt;00:39, 155.49it/s]\"}}, \"463cd1b0113b49af93ef2667f10431fd\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"4946fe208cb247308142b537f9fd5534\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"498901bec88540ed851b9c33c62b4917\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"4a2cc885f95d4c29a81f4562cb008bbd\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"4b289665ef7d48f4b5d47c2ac89351b0\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"4da1520e561740d78e3ab2632098a499\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"4fffd6b5e980482790ee0a9dab6bc9bd\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"5217380f7a184cafa172c1a137ed548d\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"55e29d51dcbf49e4b3caffd03d945c6d\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"danger\", \"description\": \" 11%\", \"layout\": \"IPY_MODEL_261cdf09822f499ab14559eb8ee0ac7a\", \"max\": 7000, \"style\": \"IPY_MODEL_4946fe208cb247308142b537f9fd5534\", \"value\": 799}}, \"58ae756ce2934e5180ebc47611436469\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"594c0b04a9364936ba3c2cd13734238c\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_382b3160cd184b88b2b1e7bc21fe01f4\", \"IPY_MODEL_9fd80264c4924dd49bec0c09f7f6a317\"], \"layout\": \"IPY_MODEL_4da1520e561740d78e3ab2632098a499\"}}, \"5c114bcb16ab4ea0919443672a2850af\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"5d1e5c17167a484d83ec5dcaa82d80a3\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"5e843d602439474a8cbae2dc06dfecf7\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"5f36d2930a6a46d3b571a8cdc8fc5e67\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_fa9733b5044d4c709c3ce303c7a135b9\", \"style\": \"IPY_MODEL_a710fc188cac48e8ac45d76aaca54331\", \"value\": \" 2204/7000 [00:25&lt;00:28, 170.37it/s]\"}}, \"5fe4890ee76c40b79680414a30f69286\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"61d8e36606e34b9b813394907b31a6ff\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_a4e4f6e0503d4def874f67e49b576bb0\", \"style\": \"IPY_MODEL_0569bc1e611a43378476986a15f5a9e9\", \"value\": \" 1000/1000 [07:00&lt;00:00,  2.38it/s]\"}}, \"62f85150290d49e8ab40a922e4b5cf5d\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"danger\", \"description\": \" 45%\", \"layout\": \"IPY_MODEL_8de5a2293dd34e7787c397b03eb634dd\", \"max\": 3000, \"style\": \"IPY_MODEL_2f095a594c614661b537260781fb6690\", \"value\": 1356}}, \"634f07da3baf4fc58bf8da8445fcf40a\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"65597a2efbb744ab951f6d3264d7603d\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"658118b4c87f4dac9912185a33bb1cb1\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_ceb2725427d84bba8d73e886f5acfd60\", \"IPY_MODEL_5f36d2930a6a46d3b571a8cdc8fc5e67\"], \"layout\": \"IPY_MODEL_8faa5aef545347978ac0049b31f44b09\"}}, \"68db20a4c1f74e809ee745c82f6447fc\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"692eb903b20842868b332365b4f9ac85\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"69652e460993413abadd4e00e7553820\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_39465a4df1804691bbcd05be4302e540\", \"IPY_MODEL_34594a065c6947a6a6140f8e28658bbb\"], \"layout\": \"IPY_MODEL_06c15915e7064e86b36cc7274f62652d\"}}, \"6a90b793bd0b441997e7a34c3396639b\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"70dcc4fe6c68466ba99c09bb483058ea\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"success\", \"description\": \"100%\", \"layout\": \"IPY_MODEL_b30be6d58684487284ea5d3f497d07b9\", \"max\": 70000, \"style\": \"IPY_MODEL_e25ed7033e7c43b08f172412e3722c99\", \"value\": 70000}}, \"747478bcf38540c0a022638b1fe4f1bb\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"success\", \"description\": \"100%\", \"layout\": \"IPY_MODEL_9f8d12e6d9a143f78281317b204191ac\", \"max\": 1000, \"style\": \"IPY_MODEL_5e843d602439474a8cbae2dc06dfecf7\", \"value\": 1000}}, \"789bc656b13a46acb7599757674ec39b\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_cfaddec7cff641c88656c8a63f1c7bca\", \"style\": \"IPY_MODEL_34203a88540742df89a1949a388ad86b\", \"value\": \" 1000/1000 [03:27&lt;00:00,  4.83it/s]\"}}, \"7bef3a7af44a4cd3b0f50a924aebc344\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"7e64df1ad5774e9c8d970d1d0cc1cdb1\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_8a9177dda6d641cc94a1e07b37ad558d\", \"style\": \"IPY_MODEL_5c114bcb16ab4ea0919443672a2850af\", \"value\": \" 7000/7000 [01:35&lt;00:00, 73.68it/s]\"}}, \"86508994c5f54d10bcbe2f60a17ebac8\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_62f85150290d49e8ab40a922e4b5cf5d\", \"IPY_MODEL_299832094d474726994bf38d140a53f7\"], \"layout\": \"IPY_MODEL_cec1e4b76a004af79536529e98337056\"}}, \"88d83fe4ef644715b9116e1ba418abc7\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_d77e08e4572f467b9ac3df83a90a3079\", \"style\": \"IPY_MODEL_5d1e5c17167a484d83ec5dcaa82d80a3\", \"value\": \" 70000/70000 [00:22&lt;00:00, 3118.56it/s]\"}}, \"8973176204974eda88cc6602f6885adc\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"success\", \"description\": \"100%\", \"layout\": \"IPY_MODEL_7bef3a7af44a4cd3b0f50a924aebc344\", \"max\": 1000, \"style\": \"IPY_MODEL_e71def60f0f74754ab0b459b6b04a0d1\", \"value\": 1000}}, \"8a9177dda6d641cc94a1e07b37ad558d\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"8de5a2293dd34e7787c397b03eb634dd\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"8faa5aef545347978ac0049b31f44b09\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"9823f91a8e2d499e8b655af0a08de3f0\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"9a40b297eb504c29af28725f28dc7ea5\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_70dcc4fe6c68466ba99c09bb483058ea\", \"IPY_MODEL_88d83fe4ef644715b9116e1ba418abc7\"], \"layout\": \"IPY_MODEL_191fa70cd5a7456390b8efa1e2199812\"}}, \"9b5fcbe152954465b4f393ead02f1bd0\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"9dff614886be46db8c9c9b4371a39611\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_c23e8ae7a285447a9e735450c03d6f1a\", \"IPY_MODEL_789bc656b13a46acb7599757674ec39b\"], \"layout\": \"IPY_MODEL_ff557540e3b3492c95da011774d80d85\"}}, \"9f8d12e6d9a143f78281317b204191ac\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"9fd80264c4924dd49bec0c09f7f6a317\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_6a90b793bd0b441997e7a34c3396639b\", \"style\": \"IPY_MODEL_eda1f91c3f29491faf846070e9639529\", \"value\": \" 3487/7000 [05:35&lt;07:29,  7.82it/s]\"}}, \"a4e4f6e0503d4def874f67e49b576bb0\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"a6183030748c4d1dbb9b84f38d07c18e\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_f233a8d7ec564fc8b43bc33282591fbc\", \"style\": \"IPY_MODEL_c2a7636b38074a239e7a42268b38819e\", \"value\": \" 1000/1000 [01:29&lt;00:00, 11.14it/s]\"}}, \"a710fc188cac48e8ac45d76aaca54331\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"a73dfd1728ad46bab131d898b1b2f707\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"a97a15f37cfc4adb9b9adf2eb0d4ba37\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HBoxModel\", \"state\": {\"children\": [\"IPY_MODEL_17a5d153712849eab6aa590553a305f1\", \"IPY_MODEL_61d8e36606e34b9b813394907b31a6ff\"], \"layout\": \"IPY_MODEL_4fffd6b5e980482790ee0a9dab6bc9bd\"}}, \"aa924e9d7b7449bdb39cd87d953cf1b9\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"abe49ced1add428eb5ac5aafc08c58e3\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_ee1af763315a4a4db555a4d183872a5f\", \"style\": \"IPY_MODEL_5217380f7a184cafa172c1a137ed548d\", \"value\": \" 1000/1000 [02:31&lt;00:00,  6.60it/s]\"}}, \"aced2c4149ac4666b843b7ea60695d9f\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"b0b69ab477ca403495ab06145aa8b695\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"b179e2a7d1dc41faab9b4ddbc962dd1f\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"b30be6d58684487284ea5d3f497d07b9\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"b8911b09f9734aa8b7c80e403b5dffd2\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_9823f91a8e2d499e8b655af0a08de3f0\", \"style\": \"IPY_MODEL_bb9ab0ea458d4b79adf49d095ffeea49\", \"value\": \" 1000/1000 [01:00&lt;00:00, 16.64it/s]\"}}, \"bb9ab0ea458d4b79adf49d095ffeea49\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"bce90aa97e7a451296ce5a420ce42158\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"bd0d13cfda43478da6eb446c72b8f9ff\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"bdb3f76b77ab4c5da94476049124a9b2\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"success\", \"description\": \"100%\", \"layout\": \"IPY_MODEL_68db20a4c1f74e809ee745c82f6447fc\", \"max\": 7000, \"style\": \"IPY_MODEL_2de7da3d53264768972844147be16644\", \"value\": 7000}}, \"c12389b37057489398bf585709aa47f0\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_184c5100ce7c492d9bd4fc22f4fe6ebd\", \"style\": \"IPY_MODEL_463cd1b0113b49af93ef2667f10431fd\", \"value\": \" 1000/1000 [03:24&lt;00:00,  4.90it/s]\"}}, \"c23e8ae7a285447a9e735450c03d6f1a\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"success\", \"description\": \"100%\", \"layout\": \"IPY_MODEL_b179e2a7d1dc41faab9b4ddbc962dd1f\", \"max\": 1000, \"style\": \"IPY_MODEL_9b5fcbe152954465b4f393ead02f1bd0\", \"value\": 1000}}, \"c2a7636b38074a239e7a42268b38819e\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"c36ec375acd04449bc03ef3511f7411a\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"ca854c0886404a5983899da0367b2411\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_692eb903b20842868b332365b4f9ac85\", \"style\": \"IPY_MODEL_4a2cc885f95d4c29a81f4562cb008bbd\", \"value\": \" 1000/1000 [01:07&lt;00:00, 14.92it/s]\"}}, \"cc239cd7b8474442a54dfa08ab834d55\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"success\", \"description\": \"100%\", \"layout\": \"IPY_MODEL_f14755f2fc7a4861a75e6ece92b50070\", \"max\": 1000, \"style\": \"IPY_MODEL_377c0371d7bf441ba0f78627af6802bb\", \"value\": 1000}}, \"ceb2725427d84bba8d73e886f5acfd60\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"danger\", \"description\": \" 31%\", \"layout\": \"IPY_MODEL_1b2c44c72d5f44f0b7c114002f5b773c\", \"max\": 7000, \"style\": \"IPY_MODEL_a73dfd1728ad46bab131d898b1b2f707\", \"value\": 2204}}, \"cec1e4b76a004af79536529e98337056\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"cfaddec7cff641c88656c8a63f1c7bca\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"d7134c4df0fa46588a08f33c75a91d98\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"HTMLModel\", \"state\": {\"layout\": \"IPY_MODEL_634f07da3baf4fc58bf8da8445fcf40a\", \"style\": \"IPY_MODEL_b0b69ab477ca403495ab06145aa8b695\", \"value\": \" 3000/3000 [03:32&lt;00:00, 14.12it/s]\"}}, \"d77e08e4572f467b9ac3df83a90a3079\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"db4667078171470daf14664eabe46a9d\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"FloatProgressModel\", \"state\": {\"bar_style\": \"success\", \"description\": \"100%\", \"layout\": \"IPY_MODEL_58ae756ce2934e5180ebc47611436469\", \"max\": 1000, \"style\": \"IPY_MODEL_65597a2efbb744ab951f6d3264d7603d\", \"value\": 1000}}, \"ddf7b57700194116a9ace472fe2ad396\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"e25ed7033e7c43b08f172412e3722c99\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"e49c91459c1b4b67a4a6981fe1ad799b\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"e71def60f0f74754ab0b459b6b04a0d1\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"ProgressStyleModel\", \"state\": {\"description_width\": \"initial\"}}, \"eda1f91c3f29491faf846070e9639529\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"ee1af763315a4a4db555a4d183872a5f\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"f14755f2fc7a4861a75e6ece92b50070\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"f233a8d7ec564fc8b43bc33282591fbc\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"f38de1c1fc244361b4da1c06e2bb6820\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"f7b64aeaa7254a5db767db15d3bfea16\": {\"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"model_name\": \"DescriptionStyleModel\", \"state\": {\"description_width\": \"\"}}, \"fa9733b5044d4c709c3ce303c7a135b9\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}, \"ff557540e3b3492c95da011774d80d85\": {\"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"model_name\": \"LayoutModel\", \"state\": {}}}, \"version_major\": 2, \"version_minor\": 0}",
      "tags": "",
      "url": "https://stanczakdominik.github.io/posts/self-organized-criticality-student-project-post-mortem/"
    },
    {
      "title": "Update on the year 2019",
      "text": "2019 has been a weird year.\nI don't feel like it's been all too good for my mental health, as I've definitely experienced burnout, mayhaps even depression. Suffice to say, the lows were low. I feel a bit like I procrastinated all there was to procrastinate.\nIt wasn't all bad, though - I'd say it was right up there in my top 25.\nIn trying to get some more work-life balance, I did get back to playing StarCraft - I'd cut it out of my life previously, along with all gaming - but I figured I do need some of that competitive drive in my life, and I still think there's a lot it can still teach me. You wouldn't drop chess from your life if you were passionate about it, right? So far, it's been treating me all right.\nAt the start of December, I started a research software job at the Institute for Plasma Physics and Laser Microfusion here in Warsaw, Poland! This is also where I'll be writing and developing my master's thesis, related to Bayesian inference for plasma diagnostics on Wendelstein 7-x, using Python and more specifically PyMC3. A step forwards for modern, open, reproducible and maintainable science software! At least, I hope so. :)\nI was planning on putting some photos, stats etc here, but I haven't found the time to get to do those yet (remember all that procrastination? Yeah, it's coming back to bite me in the predictive posterior), and Matthew Rocklin says I should write shorter blog posts more frequently, anyway, and that sounds like good advice.\nThus, to  get this out the door more quickly... onwards to 2020!",
      "tags": "",
      "url": "https://stanczakdominik.github.io/posts/update-on-the-year-2019/"
    },
    {
      "title": "Better Numba calculation of inter-particle distance matrices",
      "text": "Recently, I've been looking for efficient ways to compute a distance matrix in Python. I'm deliberately trying to implement a naive n-body simulation so as to find optimized ways of calculating those, as practice. Let's do that using Numba.\n\n\nAs usual, we're going to be using the standard Python scientific stack... and we'll also use Numba, transitioning onto the GPU next week. Let's get those imports prepped:\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport numpy as np\nimport scipy, scipy.spatial\nimport numba\nimport sys\nnp.__version__, scipy.__version__, numba.__version__, sys.version\nfrom numpy.testing import assert_allclose\n\n\n    \n\n\n\n\n\n\n\nLet's get ourselves some sample 3D position data, for twenty thousand particles:\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nN = int(1e4)\nnp.random.seed(743)\nr = np.random.random(size=(N, 3))\nr\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[2]:\n\n\n\n\n\narray([[0.83244056, 0.94442527, 0.57451672],\n       [0.09049263, 0.08428888, 0.43300003],\n       [0.29973189, 0.11463598, 0.27817412],\n       ...,\n       [0.49628111, 0.1462252 , 0.18381982],\n       [0.80535628, 0.07900376, 0.19831322],\n       [0.75236151, 0.02655101, 0.54791037]])\n\n\n\n\n\n\n\n\n\n\n\nPart I: CPU distance matrix calculations\u00b6Let's start out by following up on the 2013 results of Jake Vanderplas:\nDirect numpy summation\u00b6This is the classic approach, but with a major flaw - it allocates a lot of temporary arrays in the meantime, and that takes a while.\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \ndef pairwise_numpy(X):\n    \"\"\"\n    Reproduced from https://jakevdp.github.io/blog/2013/06/15/numba-vs-cython-take-2/\n    \"\"\"\n    return np.sqrt(((X[:, None, :] - X) ** 2).sum(-1))\npairwise_numpy_timing = %timeit -o pairwise_numpy(r)\npairwise_numpy_result = pairwise_numpy(r)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n5.02 s \u00b1 43.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n\n\n\n\n\n\n\n\n\n\nIt's nice to have it for comparison, though.\nDirect (slow) Python loop\u00b6We'll now switch over to doing things Numba-style. This means that we'll use math instead of numpy, so that the $\\sqrt{x}$ we'll doing is explicitly a scalar operation.\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nimport math\ndef scalar_distance(r, output):\n    N, M = r.shape\n    for i in range(N):\n        for j in range(N):\n            tmp = 0.0\n            for k in range(M):\n                tmp += (r[i, k] - r[j, k])**2\n            output[i,j] = math.sqrt(tmp)\noutput = np.zeros((N, N), dtype=float)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \n# warning: LONG\ndirect_summation_timeit = %timeit -o -n1 -r1 scalar_distance(r, output)\n\n# sanity check!\nassert_allclose(pairwise_numpy_result, output)\n\nprint(f\"The direct summation implementation is {direct_summation_timeit.average / pairwise_numpy_timing.average:.2f} slower than NumPy.\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n4min 6s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\nThe direct summation implementation is 49.11 slower than NumPy.\n\n\n\n\n\n\n\n\n\n\n\nAnd now, let's simply wrap this in numba.njit.\nNote that the below is equivalent to\n@numba.njit\ndef scalar_distance(...):\n    ...\n\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nnumba_jit_scalar_distance = numba.njit(scalar_distance)\nnumba_jit_timing = %timeit -o numba_jit_scalar_distance(r, output)\n\nassert_allclose(pairwise_numpy_result, output)\n\nprint(f\"Our Numba implementation is {pairwise_numpy_timing.average/numba_jit_timing.average:.2f} times faster than NumPy!\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n408 ms \u00b1 16.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\nOur Numba implementation is 12.31 times faster than NumPy!\n\n\n\n\n\n\n\n\n\n\n\nNot bad! But we can still get speedups by replacing range with numba.prange, which tells Numba that \"yes, this loop is trivially parallelizable\". To do so we use the parallel=True flag to njit:\nOptimal numba solution\u00b6\n\n\n\n\n\nIn\u00a0[7]:\n\n    \n@numba.njit(parallel=True)\ndef numba_jit_scalar_distance_parallel(r, output):\n    N, M = r.shape\n    for i in numba.prange(N):\n        for j in numba.prange(N):\n            tmp = 0.0\n            for k in range(M):\n                tmp += (r[i, k] - r[j, k])**2\n            output[i,j] = math.sqrt(tmp)\n\nnumba_jit_parallel_timing = %timeit -o numba_jit_scalar_distance_parallel(r, output)\n\nassert_allclose(pairwise_numpy_result, output)\n\nprint(f\"Using `parallel=True` grants us a further {numba_jit_timing.average/numba_jit_parallel_timing.average:.2f}x speedup.\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n105 ms \u00b1 5.98 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\nUsing `parallel=True` grants us a further 3.90x speedup.\n\n\n\n\n\n\n\n\n\n\n\nNote that I've got four cores on this laptop, so this problem is truly trivially parallelilzable. This is nice because numba.prange is actually a no-op when not using it from within numba:\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \ndef scalar_distance_prange(r, output):\n    N, M = r.shape\n    for i in numba.prange(N):\n        for j in numba.prange(N):\n            tmp = 0.0\n            for k in range(M):\n                tmp += (r[i, k] - r[j, k])**2\n            output[i,j] = math.sqrt(tmp)\n\ndirect_summation_prange_timeit = %timeit -o -n1 -r1 scalar_distance_prange(r, output)\nassert_allclose(pairwise_numpy_result, output)\nprint(f\"{direct_summation_prange_timeit.average:.5f}s vs {direct_summation_timeit.average:.5f}s.\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n4min 2s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\n242.71444s vs 246.70353s.\n\n\n\n\n\n\n\n\n\n\n\nIt's something you can just throw in \"for free\", lets you debug stuff just as easily, and once you end up turning on parallel = True, it lets speed ups kick in.\nHowever, suppose we wanted to have this run really fast. What we then could do is turn to the GPU. And this is exactly what we'll be doing next week!",
      "tags": "nbody,numba,python,simulation",
      "url": "https://stanczakdominik.github.io/posts/distance-matrix-numba/"
    },
    {
      "title": "Simple Binder usage with Sphinx-Gallery through Jupytext",
      "text": "It's been a busy week for PlasmaPy. I recently found out about Binder support in sphinx-gallery. The latter is a package that we use to\nturn python scripts with comments into Sphinx pages and Jupyter Notebooks. I figured adding that could be a nice fit for our existing example gallery .\nHowever, I quickly realized that the system in place is a bit unwieldy. Binder takes a link to an existing GitHub repository and executes .ipynb notebooks located there online. However, with sphinx-gallery, we don't have those notebooks in the repository - we have .py files with comments. The currently recommended way of setting this up with sphinx-gallery is keeping your built documentation in another repository and hosting it via something along the lines of GitHub Pages rather than ReadTheDocs, which we are currently using.\nI added the results of this investigation to sphinx-gallery's docs, but I didn't want to switch away from RTD, so I figured I'd go ahead and find another way. I think I've got something that works well enough now!\nTrigger warning: later on during this post, there may be monkeypatching of sphinx_gallery internals. Beware.\n\n\nUsing Jupytext\nThe Jupytext project is kind of like nbconvert, but two-way. It lets you turn notebooks into scripts and vice versa. The interesting thing is that, as per Jupytext's documentation, it is possible to let Binder's jupyter instance parse sphinx-gallery style .py files as jupyter notebooks. This was done in PlasmaPy#656 . First, let's instruct the in-binder Jupyter instance to parse .py files in .jupyter/jupyter_notebook_config.py via blatant copy-paste:\nc.NotebookApp.contents_manager_class = \"jupytext.TextFileContentsManager\"\nc.ContentsManager.preferred_jupytext_formats_read = \"py:sphinx\"\nc.ContentsManager.sphinx_convert_rst2md = True\nAnd then let's also add a binder/requirements.txt file that lets Binder know what Python packages to download while building the repository's image. The version I had there was pretty shoddy, as CI/setup.py/packaging errors surfaced while I was tinkering with this. Long story short, something like this should do:\n-r ../requirements/automated-documentation-tests.txt\njupytext\n.\nWhere, in didactic order:\n\njupytext should be pretty self-explanatory,\n. is the repository's package itself (here, PlasmaPy), as accessed by setup.py\n-r ../requirements/automated-documentation-tests.txt reads a pip requirements file specifying our documentation requirements. I think with a proper extras_require specified in setup.py, these two lines could be collapsed simply into .[dev] or some such. Note that -r takes a path relative to the file, thus the ../\n\nAt this point all this really is is implementing what's mentioned in Jupytext's docs. The result is as follows:\n\nBut you might notice an inconsistency in the Sphinx-rendered gallery itself: even if we were to configure docs to display Binder links they will point to a path as imagined by the current implementation in Sphinx-Gallery, such as:\nhttps://gke.mybinder.org/v2/gh/PlasmaPy/PlasmaPy/master?filepath=plasmapy/examples/auto_examples/plot_physics.ipynb\nNote the spurious auto_examples directory supposedly including an .ipynb file. This obviously doesn't work for our use case, so we'd like to be able to change the generated links somehow...\n\n\nMonkeypatching\nThis (or rather, PlasmaPy#658 ) is where it gets dirty. The solution developed in cooperation with Stuart Mumford (of SunPy fame, who contributed the idea which I implemented) is monkeypatching sphinx-gallery's link generation code. It's simple, yet effective.\nLet's use this config for sphinx-gallery:\nsphinx_gallery_conf = {\n        # path to your examples scripts\n        'examples_dirs': '../plasmapy/examples',\n        # path where to save gallery generated examples\n        'backreferences_dir': 'gen_modules/backreferences',\n        'gallery_dirs': 'auto_examples',\n        'binder': {\n                'org': 'PlasmaPy',\n                'repo': 'PlasmaPy',\n                'branch': 'master',\n                'binderhub_url': 'https://mybinder.org',\n                'dependencies': ['../binder/requirements.txt'],\n                'notebooks_dir': 'plasmapy/examples',\n        }\n}\nand add this fragment of sphinx_gallery.binder code with a modification into Sphinx's conf.py file:\n# Patch sphinx_gallery.binder.gen_binder_rst so as to point to .py file in repository\nimport sphinx_gallery.binder\ndef patched_gen_binder_rst(fpath, binder_conf, gallery_conf):\n    \"\"\"Generate the RST + link for the Binder badge.\n    ...\n    \"\"\"\n    binder_conf = sphinx_gallery.binder.check_binder_conf(binder_conf)\n    binder_url = sphinx_gallery.binder.gen_binder_url(fpath, binder_conf, gallery_conf)\n\n    # I added the line below:\n    binder_url = binder_url.replace(gallery_conf['gallery_dirs'] + os.path.sep, \"\").replace(\"ipynb\", \"py\")\n\n    rst = (\n            \"\\n\"\n            \"  .. container:: binder-badge\\n\\n\"\n            \"    .. image:: https://mybinder.org/badge_logo.svg\\n\"\n            \"      :target: {}\\n\"\n            \"      :width: 150 px\\n\").format(binder_url)\n    return rst\n\n# And then we finish our monkeypatching misdeed by redirecting sphinx-gallery to use our function:\nsphinx_gallery.binder.gen_binder_rst = patched_gen_binder_rst\nThe current gallery is located here, and an example link is https://mybinder.org/v2/gh/PlasmaPy/PlasmaPy/master?filepath=plasmapy/examples/particle_stepper.py - and you should instantly see it points to the right spot!\nObviously, it would be better to implement such link customization in sphinx-gallery itself somehow, but it's up to their maintainers to decide whether this kind of combo usage with Jupytext is in scope for their project. For now, the monkeypatch solution works decently. I'll try to update this post if that comes about.\n\n\nUpdate - requirements\n@jdkent on GitHub suggests that if the above doesn't work for you, you should make sure the Sphinx version you're using is 2 or newer.",
      "tags": "maintenance,plasmapy,python,sphinx",
      "url": "https://stanczakdominik.github.io/posts/simple-binder-usage-with-sphinx-gallery-through-jupytext/"
    },
    {
      "title": "Post mortem for my engineering thesis code, PythonPIC",
      "text": "I'm giving a presentation on this less-than-glorious subject on Friday, so\nI figured, hey, it might be a nice time to write a summary of what that old\nrepository on my GitHub page is. In a single video:\n\nAdmittedly, this post is going to be rather personal - this messy little code\nwas basically my life for a few hundreds of hours.\n\n\nThe motivation\nAt the time, I was quite enamored with Python, NumPy, the ideals of open\nsource scientific software and literate computing. I had seen\nhow most scientific software seems to be written in uglier,\nless maintainable languages and thoroughly disliked the notion that\nWe, the Scientific Community would have to keep struggling with those, no no\nno.\nSo I figured, hey, I can probably do better. I know NumPY! It's basically\nwriting FORTRAN without ever touching FORTRAN!\n\nnarrator voice: It isn't.\n\nSo I went to the amazing S\u0142awomir Jab\u0142o\u0144ski of IPPLM (to whom,\na shout out, for he is truly an amazing person without whom this work would\nhave gone nowhere). I discussed the idea with him and he seemed to like it. He\nagreed to supervise me on this idea.\nWhat happened next?\nThe breakdown\nWell, stuff happened, not least important of which was procrastination on\na scale I had never performed. One of my personal flaws is a tendency to\nisolate myself and work alone on projects that are better undertaken in groups.\nI basically started writing a framework, refactoring it endlessly, delaying\nwork on the critical bugs like the actual physics of my simulation, running\ntest cases... What I had thought would have been simple turned out not to be.\nDon't get me wrong, I learned a metric ton of Python and numerics knowledge\n- but I wasn't getting much closer, and I wasn't reaching out for feedback that\nwhat I was doing was, pretty much, crap.\nI may have had a tiny nervous\nbreakdown then. Eventually I reached out again to IPPLM and mr. Jab\u0142o\u0144ski, who\nagreed the situation is pretty bad but didn't think it worth giving up on. The\ncode ended up working after many adventures:\n|\\\n* | 207371b wat\n| |\\  \n| * | 00b554c started writing diagnostics\n| * | 3942c6e continue writing diagnostics, abandoning idea of multifunction simulation load'\n| | * e726000 revert to old simulation init, energy calculation\n| |/  \n| *   ca0742a merged\n| * 62fe945 energy still blows up\n|/  \n*   6e172d6 merged\n...\n* 5068dfb langmuir waves kinda sorta running?\n...\n* b9580ac Finish fixing tests\n* 864d27a Turn recurrent deposition into deposition on a while loop\n* 280d176 Start fixing Laser simulation\n...\n* a5c4623 Fix difficult to find bugs in longitudinal current deposition\n...\n* 080a226 what may amount to release\n\n\n\nand I was able to benchmark it nicely.\n\nThe benchmarking\nI then - only then, certainly a failure of foresight on my part - realized that\nI would need a comparable C/C++ code to benchmark my Python NumPy monstrosity\nagainst. That was not a happy thought. \nBut there was no way the low level drudgery could be avoided, so I went ahead,\nstarted learning the Eigen3 linear algebra library that has many similarities\nto NumPy - I needed something that in principle could work\nsimilarly, with mostly whole array based computations. The result of that is in\nthis repository.\nAdmittedly, to this day I'm not even one hundred percent sure it does the same\nthing! The results seemed to be correct for the test cases I did run, but I'm\nnot exactly sure I got all of the bugs. Most of the problems stem from the fact\nI used numpy.bincount for most of the current and charge deposition\nfunctionality (don't do that, by the way, as that was the least efficient part\nof the code). That functionality is lacking in Eigen (or at the very least\nI was unable to find it), so I went ahead and implemented it by hand.\nI then started benchmarking the two codes for identical initial conditions, and\nit turned out that - \n\nsuccess! PythonPIC is just as fast as the compiled C version!\n... until you compile the C code with -O, the simplest optimization flag:\n\nI didn't bother checking -O3, though a friend was happy to remind me of its\nexistence. I might thus be the proud\nauthor of the cleanest (well, not the least clean) and least efficient\nparticle-in-cell code on the planet. Gotta start somewhere, right?\nThe defense\nThere's not much to say. I went ahead, printed out a few copies of the thesis,\nrealized via review I had made mistakes in plots etc. and had to print them out\nagain. I went ahead, took my final exam, didn't get grilled too hard, got\na random exam question about - I think, as it's been a while - the Schroedinger\nequation, and poof, trust me, I'm an\nengineer of Applied\nPhysics. I'm still not sure how that happened.\nI took a break from PIC codes for a while afterwards. A snippet of PythonPIC\nlater went into PlasmaPy's\nparticle stepper capabilities. I still have a thorough dislike for that code\nand think it could be optimized.\nAll in all, I'm not sure about Python for PICs. On the one hand, they're pretty\ninevitably going to be slower than C, C++ for now. You could write the\ncomputationally intensive parts in Julia or something - that would probably\nwork and I've been itching to try my hand at that recently - but by that logic,\nyou could probably implement the whole thing in Julia anyway, because Python\nreally isn't giving you anything of value there - maybe besides analysis, but\nthat's done post-run anyway.\nBut on the other hand, not all PICs are HPiCs - and if you're just studying\nplasma physics alongside, say, Birdsall and\nLangdon,\nthen you can probably live with having your code run a little slower than the\nsickest optimal C run time. I've been able to use my code (mostly before the\nbreak... remember the part where I was adding a ton of test cases?) to\nreproduce a bunch of their results and it's worked out nicely.\nI'd like to think PythonPIC has, thus, at least some value and use. It's probably\nnot too useful for high performance and research, but you need to funnel people into those\nsomehow.\nThe lessons learned\nFocus on the physics\nNo matter how beautiful and readable your code is, if it doesn't do what it's supposed to be doing, it isn't worth a dime.\nPrecrastinate\nI've once heard a summary of estimating time for completing programming projects (unfortunately I haven't been able to find the source):\n\nfor any programming problem, make an estimate, double it, then increment the time unit to the next higher one.\n\nIt's worth starting your stuff early...\nPlan ahead and anticipate issues\n... this part tells you how early to start. Still, unexpected difficulties will\ninevitably happen - they're part of the learning process. Accept them and leave\nyourself enough time to deal with them constructively.\nReach out and talk about your problems\nThis goes both for scientific programming and for mental health. I would have\ntruly gotten nowhere if I hadn't had help from amazing people for both of\nthese.\n\nAnd... that's about it! Phew. That's a load off my chest, to be frank - the\nidea of writing this has haunted me for a while. Still, while the handling\ncould have been better, I'm happy I wrote that little code.",
      "tags": "particle-in-cell,plasma,python,simulation",
      "url": "https://stanczakdominik.github.io/posts/post-mortem-for-my-engineering-thesis-code-pythonpic/"
    },
    {
      "title": "Changing fields",
      "text": "Time to end my plasma endeavors. It turns out the field of biology has got more plasma than the entire fusion industry combined. GG, see ya on the other side!",
      "tags": "",
      "url": "https://stanczakdominik.github.io/posts/changing-fields/"
    },
    {
      "title": "On the recent \"On the Boris solver in Particle-in-cell simulations\" paper",
      "text": "I recently came across a pretty cool paper by Zenitani and Umeda named \"On the Boris solver in particle-in-cell simulation\". There are many splendid descriptions of the Boris solver on the Internet, so while I would rather not duplicate them, here's a brief overview. In PIC simulations, the Boris solver (or pusher) is the usual algorithm of choice for moving and accelerating particles in given electric and magnetic fields.\nYou may wonder, since the equations of motion are ordinary differential equations, what's wrong with using the usual Runge-Kutta 4 solver? As it turns out, that one has a pretty major flaw. It has great accuracy for short term calculations, but over time your particle's motion will lose energy. This is a deal breaker for periodic motion, and simulations of, for example, plasma waves need to conserve that energy to provide accurate results.\nBoris came up with his solver in the 1950's, and in a single sentence: the algorithm splits the acceleration via electric field into two parts and sticks a rotation about the magnetic field between them. This turns out to conserve energy and will probably come up again on this blog as I read more about symplexicity. \n\nHowever, there's a catch. There's a single basic and dense textbook for particle simulation, called \"Plasma Physics via Computer Simulation\" by Birdsall and Langdon. It has been referenced in most PIC papers I've read. The Boris solver as described by this PIC bible involves a vector quantity (following the authors we'll call this part of the Boris-B algorithm):\n$$\\vec{t} = \\frac{\\theta}{2} \\vec{b} \\tag{Boris-B}$$$\\vec{b}$ being the unit vector in the direction of the magnetic field $\\vec{B}$ and $\\theta \\sim dt |\\vec{B}|$. However, what Boris originally had in his derivation was (the Boris-A algorithm):\n$$\\vec{t} = \\tan{\\frac{\\theta}{2}} \\vec{b} \\tag{Boris-A}$$And there's a subtle difference there! Well, it's subtle if you have $\\frac{\\theta}{2} << 1$ and quickly stops being subtle if you\n\nhave large $\\theta$, which you probably shouldn't as it's proportional to the timestep\ncare about the performance of your pusher, which you probably should\n\nThe version in B&L's book is a simplification (admittedly one that B&L pointed out was being made) that incorporates a slight error in the calculation, but turns out to be a bit faster (tangents were quite expensive to calculate back then). For a very simple comparison of the two:\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nfrom math import tan\ntheta = 0.1\n\njust_division = %timeit -o theta/2\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n38 ns \u00b1 2.43 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \ntangent_division = %timeit -o tan(theta/2)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n81.2 ns \u00b1 3.39 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \n(tangent_division.average) / just_division.average\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[3]:\n\n\n\n\n\n2.138698576440618\n\n\n\n\n\n\n\n\n\n\n\nAnd that's on a modern CPU with a modern math library in a modern language! At the time of writing of B&L's book, this was indeed something people found valuable to optimize out of their code.\nWhat the authors of this paper did was take a few more steps of the calculation in the entire Boris-A algorithm and rewrite them into the Boris-C version, which turns out to be\n\njust as accurate as Boris-A (see the plots in the paper for some really neat results)\n\"only 25% slower than the Boris-B solver\"\n\"faster than the Boris-A solver\" (where Boris-A was 46% slower than Boris-B)\n\nThis is neat, so I figured we could maybe do this in Python really quickly to show how it works.\nLet's start with the classic version. We'll first include a couple of helpers:\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nimport numpy as np\n\ncharge = 1\nmass = 1\nlightspeed = 1\n\n\ndef epsilon(electric_field, timestep):\n    return charge * timestep / (2*mass) * electric_field \n\ndef gamma_from_velocity(velocity):\n    return np.sqrt(1 - np.sum((velocity / lightspeed)**2))\n\ndef gamma_from_u(u):\n    return np.sqrt(1+np.sum((u/lightspeed)**2))\n\n\n    \n\n\n\n\n\n\n\nWe can now proceed to implement the various versions of the Boris solver. I'm mostly just going through the paper and turning the equations into code, nothing crazy.u_t_minus_half is the velocity at time $t-\\Delta t /2$, as the Boris solver takes particle velocities as shifted in time: with a timestep $\\Delta t$, you get positions at $t = 0, \\Delta t, 2 \\Delta t ...$, while your velocities are defined at times $t = -\\Delta t / 2, + \\Delta t / 2, 3 \\Delta t / 2...$\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \ndef BorisA(position, u_t_minus_half, electric_field, magnetic_field, timestep):\n    # Equations 3, 6, 7a, 8, 9, 5\n    uminus = u_t_minus_half + epsilon(electric_field, timestep)  # Eq. 3\n    magfield_norm = np.linalg.norm(magnetic_field)\n    theta = charge * timestep / mass / gamma_from_u(uminus) * magfield_norm  # Eq. 6\n        \n    b = magnetic_field / magfield_norm\n    \n    t = np.tan(theta/2) * b # Eq. 7a\n    \n    uprime = uminus + np.cross(uminus, t)  # Eq. 8\n    uplus = uminus + 2/(1+(t**2).sum()) * np.cross(uprime, t)  # Eq. 9\n    u_t_plus_half = uplus + epsilon(electric_field, timestep) # Eq. 5\n    new_position = u_t_plus_half / gamma_from_u(u_t_plus_half) * timestep + position # Eq. 1\n    return new_position, u_t_plus_half \n\ndef BorisB(position, u_t_minus_half, electric_field, magnetic_field, timestep):\n    # 3, 7b, 8, 9, 5\n    uminus = u_t_minus_half + epsilon(electric_field, timestep)  # Eq. 3\n    \n    # Eq. 7a\n    t = charge * timestep / (2 * mass * gamma_from_u(uminus)) * magnetic_field\n    \n    uprime = uminus + np.cross(uminus, t)  # Eq. 8\n    uplus = uminus + 2/(1+(t**2).sum()) * np.cross(uprime, t)  # Eq. 9\n    u_t_plus_half = uplus + epsilon(electric_field, timestep) # Eq. 5\n    new_position = u_t_plus_half / gamma_from_u(u_t_plus_half) * timestep + position # Eq. 1\n    return new_position, u_t_plus_half \n    \ndef BorisC(position, u_t_minus_half, electric_field, magnetic_field, timestep):\n    # 3, 6, 11, 12, 5\n    uminus = u_t_minus_half + epsilon(electric_field, timestep)  # Eq. 3\n    magfield_norm = np.linalg.norm(magnetic_field)\n    theta = charge * timestep / mass / gamma_from_u(uminus) * magfield_norm  # Eq. 6\n    \n    b = magnetic_field / magfield_norm\n    \n    u_parallel_minus = np.dot(uminus, b) * b # Eq. 11\n    uplus = u_parallel_minus + (uminus - u_parallel_minus) * np.cos(theta) + np.cross(uminus, b) * np.sin(theta) # Eq. 12\n    u_t_plus_half = uplus + epsilon(electric_field, timestep) # Eq. 5\n    new_position = u_t_plus_half / gamma_from_u(u_t_plus_half) * timestep + position # Eq. 1\n    return new_position, u_t_plus_half \n\n\n    \n\n\n\n\n\n\n\nWe can now start implementing the authors' test cases as seen in the paper. We'll first define a helper plotting function:\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \ndef plot(r, v, trajectory_format = \".:\", timeseries_format = \".--\"):\n    x, y, z = r.T\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n    axes[0,0].plot(x, timeseries_format, label=\"x\")\n    axes[0,0].plot(y, timeseries_format, label=\"y\")\n    axes[0,0].plot(z, timeseries_format, label=\"z\")\n    axes[0,0].set_xlabel(\"Iteration\")\n    axes[0,0].legend(loc='best')\n    \n    axes[1,0].plot(x, y, trajectory_format)\n    axes[1,0].set_xlabel(\"X\")\n    axes[1,0].set_ylabel(\"Y\")\n    \n    vx, vy, vz = v.T\n    axes[0, 1].plot(vx, timeseries_format, label=\"Vx\")\n    axes[0, 1].plot(vy, timeseries_format, label=\"Vy\")\n    axes[0, 1].plot(vz, timeseries_format, label=\"Vz\")\n    axes[0, 1].legend(loc='best')\n    axes[0, 1].set_xlabel(\"Iteration\")\n    axes[0, 1].set_ylabel(\"Velocity\")\n                      \n    axes[1, 1].plot(vx, vy, trajectory_format)\n    axes[1, 1].set_xlabel(\"X Velocity\")\n    axes[1, 1].set_ylabel(\"Y Velocity\")\n    plt.tight_layout()\n    return r, v\n\n\n    \n\n\n\n\n\n\n\nAnd now we can start to implement the first test case:\nMovement in constant crossed electric and magnetic fields\u00b6\n\n\n\n\n\nIn\u00a0[7]:\n\n    \ndef drift(pusher, E=1, B=1):\n    electric_field = np.array([E, 0, 0])\n    magnetic_field = np.array([0, 0, B])\n    \n    # initial conditions\n    u_t_minus_half = np.array([1, 0, 0])\n    position = np.zeros(3)\n    timestep = np.pi/6\n    \n    # I'm taking this a bit longer than the authors, so that the plots look nicer\n    t = np.arange(0, 120/np.pi, timestep) \n    \n    positions = []\n    velocities = []\n    for i in t:\n        positions.append(position)\n        velocities.append(u_t_minus_half)\n        position, u_t_minus_half = pusher(position, u_t_minus_half, electric_field, magnetic_field, timestep)\n\n    \n    r = np.array(positions)\n    v = np.array(velocities)\n    return r, v\n\nplot(*drift(BorisA, E=0, B=1));\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat looks reasonable.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nplot(*drift(BorisB, E=0, B=1));\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nplot(*drift(BorisC, E=0, B=1));\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPretty indistinguishable from the BorisA case! In fact, that's what the authors claim and what we can check numerically:\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \nfor name, array_A, array_B, array_C in zip([\"position\", \"velocity\"], drift(BorisA), drift(BorisB), drift(BorisC)):\n    print(f\"BorisA and BorisC {'' if np.allclose(array_A, array_C, atol=1e-20, rtol=1e-15) else 'DO NOT '}agree on {name} for rotation.\")\n    print(f\"BorisB and BorisC {'' if np.allclose(array_B, array_C, atol=1e-20, rtol=1e-15) else 'DO NOT '}agree on {name} for rotation.\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nBorisA and BorisC agree on position for rotation.\nBorisB and BorisC DO NOT agree on position for rotation.\nBorisA and BorisC agree on velocity for rotation.\nBorisB and BorisC DO NOT agree on velocity for rotation.\n\n\n\n\n\n\n\n\n\n\n\nI went through the different cases presented for this part in the paper, and they seem to agree as well. Let's reproduce another example, the $\\vec{E} \\times \\vec{B}$ drift. I won't show the BorisB plot here, as it doesn't visually differ, though the difference is there:\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \nborisC_drift = plot(*drift(BorisC, E=1, B=1))\nborisB_drift = drift(BorisB, E=1, B=1)\nborisA_drift = drift(BorisA, E=1, B=1)\nfor name, array_A, array_B, array_C in zip([\"position\", \"velocity\"], borisA_drift, borisB_drift, borisC_drift):\n    print(f\"BorisA and BorisC {'' if np.allclose(array_A, array_C, atol=1e-20, rtol=1e-15) else 'DO NOT '}agree on {name} on the ExB drift.\")\n    print(f\"BorisB and BorisC {'' if np.allclose(array_B, array_C, atol=1e-20, rtol=1e-15) else 'DO NOT '}agree on {name} on the ExB drift.\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nBorisA and BorisC agree on position on the ExB drift.\nBorisB and BorisC DO NOT agree on position on the ExB drift.\nBorisA and BorisC agree on velocity on the ExB drift.\nBorisB and BorisC DO NOT agree on velocity on the ExB drift.\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLong term stability tests\u00b6The authors define this as a long time run in the following fields:\n$$ \\phi = \\frac{0.01}{\\sqrt{x2 + y2)}} $$$$ \\vec{B} = \\sqrt{x2 + y2} $$with $\\vec{E} = -\\nabla \\phi$ as usual. Let's just calculate the derivative with SymPy really quickly here:\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \nfrom sympy.abc import x, y\nphi = 0.01 * (x**2 + y**2)**-0.5\nphi\n\nfrom sympy import lambdify\n\nEx = -phi.diff(x)\nEy = -phi.diff(y)\nEx = lambdify((x, y), Ex)\nEy = lambdify((x, y), Ey)\n\ndef stability(pusher, time_range=8e2):\n    u_t_minus_half = np.array([0.1, 0, 0])\n    position = np.array([0.9, 0, 0])\n    timestep = np.pi/10\n    t = np.arange(0, time_range, timestep)    \n    \n    positions = []\n    velocities = []\n    for i in t:\n        x, y, z = position\n        magnetic_field = np.array([0, 0, np.sqrt(x**2 + y**2)])\n        electric_field = np.array([Ex(x, y), Ey(x, y), 0])\n        positions.append(position)\n        velocities.append(u_t_minus_half)\n        position, u_t_minus_half = pusher(position, u_t_minus_half, electric_field, magnetic_field, timestep)\n    \n    r = np.array(positions)\n    v = np.array(velocities)\n    return r, v\n   \n\nplot(*stability(BorisA), trajectory_format=\".\");\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \nplot(*stability(BorisC), trajectory_format=\".\");\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \nplot(*stability(BorisB), trajectory_format=\".\");\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote how the inner side of the velocity space trajectory becomes circular for the BorisB case and keeps making a neat pattern in the BorisA and BorisC cases!\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \nrtol = 1e-9\nfor name, array_A, array_B, array_C in zip([\"position\", \"velocity\"], stability(BorisA), stability(BorisB), stability(BorisC)):\n    print(f\"BorisA and BorisC {'' if np.allclose(array_A, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n    print(f\"BorisB and BorisC {'' if np.allclose(array_B, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nBorisA and BorisC agree on position for long term stability for relative tolerance 1e-09.\nBorisB and BorisC DO NOT agree on position for long term stability for relative tolerance 1e-09.\nBorisA and BorisC agree on velocity for long term stability for relative tolerance 1e-09.\nBorisB and BorisC DO NOT agree on velocity for long term stability for relative tolerance 1e-09.\n\n\n\n\n\n\n\n\n\n\n\nDo note that there does seem to be some long term error creeping in, as I had to lower rtol:\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \nrtol = 1e-10\nfor name, array_A, array_B, array_C in zip([\"position\", \"velocity\"], stability(BorisA), stability(BorisB), stability(BorisC)):\n    print(f\"BorisA and BorisC {'' if np.allclose(array_A, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n    print(f\"BorisB and BorisC {'' if np.allclose(array_B, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nBorisA and BorisC agree on position for long term stability for relative tolerance 1e-10.\nBorisB and BorisC DO NOT agree on position for long term stability for relative tolerance 1e-10.\nBorisA and BorisC DO NOT agree on velocity for long term stability for relative tolerance 1e-10.\nBorisB and BorisC DO NOT agree on velocity for long term stability for relative tolerance 1e-10.\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \nrtol = 1e-15\nfor name, array_A, array_B, array_C in zip([\"position\", \"velocity\"], stability(BorisA), stability(BorisB), stability(BorisC)):\n    print(f\"BorisA and BorisC {'' if np.allclose(array_A, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n    print(f\"BorisB and BorisC {'' if np.allclose(array_B, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nBorisA and BorisC DO NOT agree on position for long term stability for relative tolerance 1e-15.\nBorisB and BorisC DO NOT agree on position for long term stability for relative tolerance 1e-15.\nBorisA and BorisC DO NOT agree on velocity for long term stability for relative tolerance 1e-15.\nBorisB and BorisC DO NOT agree on velocity for long term stability for relative tolerance 1e-15.\n\n\n\n\n\n\n\n\n\n\n\nStill, the results look all right, especially if we were to overlay the trajectories. Let's calculate a bunch of long-time trajectories (like the authors do) and make a quick adjustment to the plotting function:\n\n\n\n\n\n\nIn\u00a0[18]:\n\n    \nborisA_stability = stability(BorisA, time_range=2e5)\nborisB_stability = stability(BorisB, time_range=2e5)\nborisC_stability = stability(BorisC, time_range=2e5)\n\ndef plot_shared(*tuples, r_format=\",\", v_format = \".\", alpha=0.1, start_at):\n    fig, axes = plt.subplots(ncols=2, figsize=(12, 7))\n    for r, v in tuples:\n        x, y, z = r[int(start_at):].T\n        axes[0].set_title(\"x-y position trajectory\")\n        axes[0].plot(x, y, r_format, alpha=alpha)\n        vx, vy, vz = v[int(start_at):].T\n        \n        axes[1].set_title(\"x-y velocity trajectory\")\n        axes[1].plot(vx, vy, v_format, alpha=alpha)\n    plt.tight_layout()\n\n\n    \n\n\n\n\n\n\nIn\u00a0[24]:\n\n    \nplot_shared(borisA_stability,\n            borisC_stability,\n            r_format=\".\",\n            v_format=\".\",\n            alpha=0.8,\n            start_at = len(borisA_stability[0]) * 0.99\n           )\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[26]:\n\n    \nplot_shared(borisB_stability,\n            borisC_stability,\n            r_format=\".\",\n            v_format=\".\",\n            alpha=0.8,\n            start_at = len(borisA_stability[0]) * 0.99\n           )\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmark\u00b6And to finalize, let's benchmark the results, to see if BorisC is indeed faster than BorisA:\n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \nelectric_field = np.array([1, 0, 0])\nmagnetic_field = np.array([0, 0, 1])\n\n# initial conditions\nu_t_minus_half = np.array([1, 0, 0])\nposition = np.zeros(3)\ntimestep = np.pi/6\n\n%timeit BorisA(position, u_t_minus_half, electric_field, magnetic_field, timestep)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n101 \u00b5s \u00b1 1.29 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \n%timeit BorisB(position, u_t_minus_half, electric_field, magnetic_field, timestep)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n89.9 \u00b5s \u00b1 2.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \n%timeit BorisC(position, u_t_minus_half, electric_field, magnetic_field, timestep)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n71.8 \u00b5s \u00b1 2.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\n\n\n\n\n\n\n\n\n\n\n... and this clearly points to some inefficiency in my BorisB implementation. BorisC does seem to beat BorisA, though, so I guess we didn't manage to falsify the result!\n\n\n\n\n \n\n\n{\"state\": {}, \"version_major\": 2, \"version_minor\": 0}",
      "tags": "paper,particle-in-cell,plasma,python,simulation",
      "url": "https://stanczakdominik.github.io/posts/on-the-recent-on-the-boris-solver-in-particle-in-cell-simulations-paper/"
    },
    {
      "title": "Particle in Cell methods",
      "text": "I think it might finally be about time to do some plasma physics\ndiscussion on this blog, stay true to the name and so on\u2026\nBasically the only actual \u201cscientific\u201d work I have actually done with\nplasmas up until now is writing a PIC simulation, PIC standing for\nParticle-in-Cell. I thought I would take this opportunity to explain in\nmy own words what the concept is - I think it\u2019s a clever one.\n\nThere are many reasons why you might want to simulate a plasma.\nSimulations are often way cheaper than making a tokamak and causing the\nplasma to develop turbulence, or sending out a probe to watch check\nsolar flares for traces of magnetic reconnection. There\u2019s also the case\nof needing simulations to understand and explain your experimental\nresults. For now, however, let\u2019s just assume you have a burning desire\nto make a few pretty plots and animations like\nthese using data that\nyou don\u2019t have simple access to via experiment.\n\n\n\n\nPlasma simulation in general\nPlasmas are, in general, difficult to simulate. Many of the interesting\nprocesses in plasmas that you would like to simulate occur far from\nequilibrium, both spatial and thermal. For example, you\u2019d like to\nsimulate interactions between a plasma and a laser pulse (this is, in\nfact, what I did for my engineering\nthesis). This is a\nmassively non-equilibrium process. This mostly rules out fluid\nsimulations such as two-fluid and magnetohydrodynamics, which are based\non averaging the Vlasov\nequation over all\npossible velocities. You could, in theory, use the Vlasov equation\ndirectly - but that\u2019s a pretty darn high dimensional problem to be\nsolving a PDE on (though something I most certainly want to try my hand\nat, one of these days!).\nSuppose you want to take another approach. Maybe you like your\nNewtonian, old fashioned dynamical particle trajectory ODEs, you\u2019ve\ndabbled in some N-body simulations, maybe you\u2019ve done a bit of molecular\ndynamics. You could imagine putting a bunch of charged particles into a\nsimulation, calculating forces between those directly and letting them\nevolve over time.\nUnfortunately, there is a major flaw in that plan. You\u2019ve got long range\n(Coulomb) \\(r{-2}\\) interactions between huge numbers of particles,\nso you cannot use the neat trick common in molecular dynamics of only\nincluding a few neighbor particles in your simulation. This means your\nsimulation will scale as full \\(O(N2)\\) in the number of particles\nif you do that (though there have been attempts at doing that\nrecently).\nI guess you could also try a Barnes-Hut treecode of some sort, and that\nalso appears to have been\ndone\n- that doesn\u2019t seem like it\u2019s caught on, though, whatever the reason.\nWe\u2019ve now set the stage and can move on to the main attraction\u2026\n\n\nThe particle in cell method\nThe logic for a PIC is as follows, starting from the simple molecular\ndynamics or N-body framework:\n\nIf we were to know the force on each particle for every time step, we\ncould push them - update their velocities and positions as usual,\nin \\(O(N)\\) steps. Each particle is assumed independent of\nothers.\nIt\u2019s hard to calculate the forces directly in \\(O(N2)\\) steps.\nOn the other hand, It\u2019s relatively easier to solve a PDE for the\nelectromagnetic field given a charge and current distribution. The\nparticles we\u2019re moving are charged, so we can do the following\ntranslation:\n\nParticle positions \\(\\implies\\) charge distribution\nParticle positions and velocities \\(\\implies\\) current\ndistribution\n\nThis means we could deposit the particles onto a grid or mesh by\nsome kind of interpolation. We can also set the grid size so that\nmany particles go into a single grid cell: this implies that the\nnumber of grid cells is much lower than our particle count. That, in\nturn, fits our assumptions for plasmas 1. A picture is worth a\nthousand words, so here\u2019s a very basic example of a particle\u2019s charge\nwith a linear (triangular, and thus, centered on the middle)\ndistribution being split between three cells.\n\n\n\nOnce we have the charge and current distribution on our grid, we can\nuse those quantities to solve Maxwell\u2019s equations for the\nelectromagnetic field. You could, for example, use a spectral method\nor a relaxation algorithm, like conjugate gradients.\nOnce we know the fields at the grid cell locations, we can gather\nthe field from those to the particle locations. Remember step 1,\nwhere we wished for forces - readily available given fields - at\nparticle locations? Well, here we go, wish granted!\ngoto 1\n\nAnd that\u2019s it, the particle-in-cell method in a nutshell. Of course,\nlogically it makes more sense to start from 2. (as you would usually\nstart your simulation with a set of initial conditions for the particle\npositions, velocities and maybe external fields), but to me it\u2019s cleaner\nnarratively to think of the algorithm in this order.\n\nThe advantages\n\nIt\u2019s close to fundamental physics and thus understandable! You get a\nfull picture of what each of the particles does, how the fields\nbehave, while making very few assumptions.\nIt\u2019s lightning fast! The \\(O(N2)\\) force calculation is reduced\nto the complexity of your three replacement steps. While you can\nexpect deposition and gathering to be roughly \\(O(Nm)\\)\n(\\(m\\) being the number of cells), \\(N\\) is much larger than\n\\(m\\), and the field solver is going to scale independently of\n\\(N\\) - so that\u2019s still a massive gain over direct summation.\nIt\u2019s easy to parallelize! Each particle is essentially independent\nfor the pushing step (as they only interact with each other via\nfields), so those movements are trivially parallel. Grid operations\ncan also be done in parallel (though admittedly I haven\u2019t looked into\nthat much, yet - I fully intend to do so).\n\nAnd of course, no description of a simulation method is complete\nwithout\u2026\n\n\nThe disadvantages\n\nThe method is mostly explicit, so that limits your time step and grid\nsize quite a lot. Otherwise, you get spurious instabilities.\nStatistical noise makes life a pain when you\u2019re working on PIC\nsimulations, precisely because you\u2019re modeling your large numbers of\nreal particles with fewer virtual discrete ones. The trick seems to\nbe increasing the particle numbers, but Wikipedia\nclaims\nthat this source of error is more figured out for traditional grid\nmethods. In a way, this also means PICs are a prime target for GPUs,\nas exhibited by\nPIConGPU.\n\nStill, PICs are used in many awesome applications, such as plasma\nturbulence research, and their parallelizability means they\u2019re only\ngoing to get more important in the coming exascale computing era.\nI\u2019ll be writing a few follow-up posts going over particular aspects of\nPIC codes - tricks I\u2019ve picked up along the way, etc. Stay tuned!\n\nReferences\n\nRelativistic kinetic turbulence\nvideo by Joonas\nN\u00e4ttil\u00e4, using the plasmabox\ncode.\nPythonPIC, my\nless-than-amazing engineering thesis code.\nVlasov Equation - Wikipedia\nPlasma simulation via molecular dynamics\nexample.\nBarnes-Hut plasma simulation\nexample\nParticle in Cell applications -\nWikipedia\nGPU PIC\nPIConGPU\n\n\n1\nIf you want to learn more about those, I don\u2019t feel like I can give\nthis subject justice better than chapter 1-1 of Birdsall and\nLangdon\u2019s seminal Plasma Physics via Computer Simulation text.",
      "tags": "particle-in-cell,pic,plasma",
      "url": "https://stanczakdominik.github.io/posts/particle-in-cell-methods/"
    },
    {
      "title": "First dive into Julia",
      "text": "I'm currently sitting in at a tutorial on the basics of Julia and parallel processing therein by Bogumi\u0142 Kami\u0144ski. This is actually my first dive into Julia, so I thought I would write it down on-the-go here!\n\n\nLet's jump right into it:\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nR = randn(10000, 10000)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[1]:\n\n\n\n\n\n10000\u00d710000 Array{Float64,2}:\n -0.324424   -0.565317    0.74943    \u2026  -0.518865    -0.0841272   0.935276 \n  2.7421      0.127783   -0.406756       1.69075      1.58605     0.302112 \n  2.00937    -0.36474    -1.6031         2.46846     -0.319774   -0.362626 \n  1.0957     -0.328512    0.0765665      0.551588    -0.63376    -0.642072 \n -1.5761      0.0990041   0.649661       0.123745     1.53702     0.748066 \n  0.0294794   0.841421    0.935812   \u2026  -0.124979    -0.0319694  -0.308331 \n  2.4428     -0.0981946   2.16323       -1.74004     -0.838027   -0.562755 \n -0.362584   -0.342403    1.11269       -1.99102      2.13044     1.05996  \n -0.85741     0.224304    0.89256       -0.357627    -0.25959     0.271416 \n  1.02282    -0.470008    1.75296        1.34871     -0.16343     0.194525 \n -0.357741    0.252059   -1.02996    \u2026  -0.125655    -1.20237     0.0220102\n  0.793983    0.334861   -0.628246      -0.768169     1.08063    -0.870663 \n -0.111529   -0.557087    0.714131      -0.0785655    0.577348   -0.659775 \n  \u22ee                                  \u22f1                                     \n  0.454754    0.905449   -1.04019       -2.14169     -0.830821    0.363394 \n  0.165472   -0.099097    1.58675       -0.314269    -0.500922   -2.24592  \n -0.74685     0.854795   -0.606661   \u2026   0.390252    -1.45657     1.22648  \n -0.0369208  -0.139647    1.26695       -0.00442996  -2.24374     0.348733 \n  0.620604   -0.835141   -1.59741       -0.026424    -0.491713    0.705191 \n -2.49094     0.471711    0.677353       0.51443     -0.234433    1.61501  \n -0.600199    0.907787   -0.0977633      1.39034     -1.20908     1.06054  \n -1.26894     0.718772    0.334036   \u2026   0.994015    -1.28285    -2.15419  \n -0.411544   -0.0794345   1.58904        1.14895      0.0363      2.14895  \n -0.437881   -0.451166   -0.0647529     -0.276704     0.392206   -0.128466 \n  0.86126     0.774654    0.429458      -2.03196     -0.371577    0.547281 \n -1.07246     0.421693   -0.244775      -0.479385    -0.858759   -0.300843 \n\n\n\n\n\n\n\n\n\n\n\nOkay, that did what you'd expect. There's apparently a help statement that works inversely from what you'd expect from IPython:\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \n?randn\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nsearch: randn rand transcode macroexpand @macroexpand1 @macroexpand\n\n\n\n\n\n\n\n    Out[2]:\n\n\n\n\n\nrandn([rng=GLOBAL_RNG], [T=Float64], [dims...])\nGenerate a normally-distributed random number of type T with mean 0 and standard deviation 1. Optionally generate an array of normally-distributed random numbers. The Base module currently provides an implementation for the types Float16, Float32, and Float64 (the default), and their Complex counterparts. When the type argument is complex, the values are drawn from the circularly symmetric complex normal distribution.\nExamples\u00b6\njldoctest\njulia> rng = MersenneTwister(1234);\n\njulia> randn(rng, ComplexF64)\n0.6133070881429037 - 0.6376291670853887im\n\njulia> randn(rng, ComplexF32, (2, 3))\n2\u00d73 Array{Complex{Float32},2}:\n -0.349649-0.638457im  0.376756-0.192146im  -0.396334-0.0136413im\n  0.611224+1.56403im   0.355204-0.365563im  0.0905552+1.31012im\n\n\n\n\n\n\n\n\n\n\n\n\nWhat about global help?\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \n?\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nsearch: \u22bb \u228b \u228a \u2289 \u2288 \u2287 \u2286 \u2265 \u2264 \u2262 \u2261 \u2260 \u2249 \u2248 \u222a \u2229 \u221b \u221a \u2218 \u220c \u220b \u2209 \u2208 \u212f \u03c0 \u00f7 ~ |  \\ > < : / - +\n\n\n\n\n\n\n\n    Out[3]:\n\n\n\n\nWelcome to Julia 1.1.0. The full manual is available at\n\nhttps://docs.julialang.org/\nas well as many great tutorials and learning resources:\n\nhttps://julialang.org/learning/\nFor help on a specific function or macro, type ? followed by its name, e.g. ?cos, or ?@time, and press enter. Type ; to enter shell mode, ] to enter package mode.\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \n;ls\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nmnist.zip\nTutorial1.ipynb\n\n\n\n\n\n\n\n\n\n\n\nWe can use @time to benchmark this randn command:\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \n@time randn(10000, 10000);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n  0.813875 seconds (6 allocations: 762.940 MiB, 1.56% gc time)\n\n\n\n\n\n\n\n\n\n\n\nFor comparison:\nIn [3]: %timeit np.random.normal(size=(10000, 10000))             \n3.8 s \u00b1 25.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nAnd the presenter's R attempt took 5.81 using system.time. Wew, this is pretty fast.\n\n\n\n\n\n\n\nTo start with, we're analyzing a function found on StackOverflow that sums over the lower triangular part of a matrix (apparently, the code is pretty bad):\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nfunction upsum(M); n = size(M)[1]; sum = 0\n    for i = 1:n-1\n        for j = i+1:n\n            sum = sum + M[i,j]\n        end\n    end\n    return sum\nend\n\nupsum(R)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[6]:\n\n\n\n\n\n-7802.649783031088\n\n\n\n\n\n\n\n\n\n\n\nLet's check the performance:\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \n%timeit upsum(R);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\nThe analogue of IPython's %time statement (also %timeit) in Julia is @time statement.  The analogue of %%time ...code... is\n\n@time begin\n    ...code...\nend\nNote, however, that you should put all performance-critical code into a function, avoiding global variables, before doing performance measurements in Julia; see the performance tips in the Julia manual.\nThe @time macro prints the timing results, and returns the value of evaluating the expression.  To instead return the time (in seconds), use @elapsed statement.\nFor more extensive benchmarking tools, including the ability to collect statistics from multiple runs, see the BenchmarkTools package.\n\n\n\n\n\n\n\n\n\n\n\n\n... All right, I'm starting to like this cheeky little language. Trying again:\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \n@time upsum(R);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n  0.464638 seconds (5 allocations: 176 bytes)\n\n\n\n\n\n\n\n\n\n\n\nTo compare that with Python:\nIn [8]: %timeit np.sum(np.tril(R))                                \n245 ms \u00b1 45.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nWell, that was faster, but we can improve the Julia code. Let's first look at the inbuilt sum function:\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nsum\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[9]:\n\n\n\n\n\nsum (generic function with 13 methods)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \n@time sum(R);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n  0.090729 seconds (89.03 k allocations: 4.748 MiB, 10.32% gc time)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \n@time sum(R);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n  0.100551 seconds (5 allocations: 176 bytes)\n\n\n\n\n\n\n\n\n\n\n\nOkay, now this is badass. Julia is dynamically compiled - it's as if Numba came out of Python and became its own language. Apparently there are ways of avoiding the first-call overhead, but this is somehow more advanced.\nNote that all compiled-function cache is cleared on Julia restarts!\nTo compare with Python:\nIn [9]: %timeit np.sum(R)                                         \n53 ms \u00b1 3.67 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nNot too shabby for the ol' snake!\nLet's try to improve the function, though:\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \nfunction uppersum(M)\n    n = size(M, 1)\n    s = zero(eltype(M))  # a zero hard-typed as the same type as the entry of the matrix\n    # eltype stands for ELement TYPE - this is now fully generic\n    for i in 2:n        # Julia uses column-major storage order - faster to traverse any matrix in Julia column-wise\n        @simd for j in 1:(i-1)    # if I know I'm accessing a contiguous block of memory, I can tell Julia that using @simd\n            @inbounds s += M[j, i]    # ignore bound checking and just access memory C-style \n        end\n    end\n    s\nend\n\nuppersum(R)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[12]:\n\n\n\n\n\n-7802.6497830305125\n\n\n\n\n\n\n\n\n\n\n\nWe can look at these @simd and @inbouds annotations:\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \n?@simd\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[13]:\n\n\n\n\n\n@simd\nAnnotate a for loop to allow the compiler to take extra liberties to allow loop re-ordering\n!!! warning\n    This feature is experimental and could change or disappear in future versions of Julia. Incorrect use of the @simd macro may cause unexpected results.\nThe object iterated over in a @simd for loop should be a one-dimensional range. By using @simd, you are asserting several properties of the loop:\n\nIt is safe to execute iterations in arbitrary or overlapping order, with special consideration for reduction variables.\nFloating-point operations on reduction variables can be reordered, possibly causing different results than without @simd.\n\nIn many cases, Julia is able to automatically vectorize inner for loops without the use of @simd. Using @simd gives the compiler a little extra leeway to make it possible in more situations. In either case, your inner loop should have the following properties to allow vectorization:\n\nThe loop must be an innermost loop\nThe loop body must be straight-line code. Therefore, @inbounds is   currently needed for all array accesses. The compiler can sometimes turn   short &&, ||, and ?: expressions into straight-line code if it is safe   to evaluate all operands unconditionally. Consider using the ifelse   function instead of ?: in the loop if it is safe to do so.\nAccesses must have a stride pattern and cannot be \"gathers\" (random-index   reads) or \"scatters\" (random-index writes).\nThe stride should be unit stride.\n\n!!! note\n    The @simd does not assert by default that the loop is completely free of loop-carried memory dependencies, which is an assumption that can easily be violated in generic code. If you are writing non-generic code, you can use @simd ivdep for ... end to also assert that:\n\nThere exists no loop-carried memory dependencies\nNo iteration ever waits on a previous iteration to make forward progress.\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \n?@inbounds\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[14]:\n\n\n\n\n\n@inbounds(blk)\nEliminates array bounds checking within expressions.\nIn the example below the in-range check for referencing element i of array A is skipped to improve performance.\nfunction sum(A::AbstractArray)\n    r = zero(eltype(A))\n    for i = 1:length(A)\n        @inbounds r += A[i]\n    end\n    return r\nend\n\n!!! warning\n    Using @inbounds may return incorrect results/crashes/corruption for out-of-bounds indices. The user is responsible for checking it manually. Only use @inbounds when it is certain from the information locally available that all accesses are in bounds.\n\n\n\n\n\n\n\n\n\n\n\n\nRight! Let's time this implementation:\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \n@time uppersum(R);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n  0.054047 seconds (5 allocations: 176 bytes)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \nfunction uppersum_boundcheck(M)\n    n = size(M, 1)\n    s = zero(eltype(M))  # a zero hard-typed as the same type as the entry of the matrix\n    # eltype stands for ELement TYPE - this is now fully generic\n    for i in 2:n        # Julia uses column-major storage order - faster to traverse any matrix in Julia column-wise\n        @simd for j in 1:(i-1)    # if I know I'm accessing a contiguous block of memory, I can tell Julia that using @simd\n            s += M[j, i]    # ignore bound checking and just access memory C-style \n        end\n    end\n    s    # this is sufficient for a `return` statement\nend\n\nuppersum(R)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[16]:\n\n\n\n\n\n-7802.6497830305125\n\n\n\n\n\n\n\n\n\n\n\nLet's see what kind of gain we get from losing boundchecking:\n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \n@time uppersum_boundcheck(R);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n  0.115098 seconds (51.16 k allocations: 2.663 MiB)\n\n\n\n\n\n\n\n\n\n\n\nInteresingly, Julia apparently uses LLVM in the background.\nGoing parallel\u00b6The idea is: we have a triangle and we want to split it into pieces of equal \"mass\". This is done in the code below, via an instruction that is relatively magical to me right now.\nFor threading to work, note that as described in the docs, you need to set the environment variable JULIA_NUM_THREADS. export JULIA_NUM_THREADS=4 in .bashrc worked fine for me.\n\n\n\n\n\n\nIn\u00a0[34]:\n\n    \nusing Base.Threads\n\nfunction upsum_threads(M)\n    n = size(M, 1)\n    chunks = nthreads()\n    sums = zeros(eltype(M), chunks)  # separate subsum for each thread\n    chunkend = round.(Int, n*sqrt.((1:chunks) ./ chunks))   #split jobs so that each thread has approx. same number of numbers to add\n    @assert minimum(diff(chunkend)) > 0\n    chunkstart = [2; chunkend[1:end-1] .+ 1]\n    @threads for job in 1:chunks     # tell Julia that this part is safe for threading\n        s = zero(eltype(M))\n        for i in chunkstart[job]:chunkend[job]\n            @simd for j in 1:(i-1)\n                @inbounds s += M[j, i]\n            end\n        end\n        sums[job] = s\n    end\n    return sum(sums)\nend\n\nupsum_threads(R)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[34]:\n\n\n\n\n\n-7802.649783030595\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[36]:\n\n    \n@time upsum_threads(R);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n  0.037879 seconds (35 allocations: 2.000 KiB)\n\n\n\n\n\n\n\n\n\n\n\nOkay, now this is faster than the Numpy. I'm reasonably impressed - but confused as to that one magical line, though. Let's dig into it.\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \nchunks = 4\nn = 10000\nround.(Int, n*sqrt.((1:chunks) ./ chunks))\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[20]:\n\n\n\n\n\n4-element Array{Int64,1}:\n  5000\n  7071\n  8660\n 10000\n\n\n\n\n\n\n\n\n\n\n\nHuh. Digging deeper:\n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \n(1:chunks)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[21]:\n\n\n\n\n\n1:4\n\n\n\n\n\n\n\n\n\n\n\nOkay, this is a range...\n\n\n\n\n\n\nIn\u00a0[38]:\n\n    \n(1:chunks) ./ chunks\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[38]:\n\n\n\n\n\n0.25:0.25:1.0\n\n\n\n\n\n\n\n\n\n\n\nAnd this is where it starts to hit me, as the presenter introduces the collect command:\n\n\n\n\n\n\nIn\u00a0[39]:\n\n    \ncollect((1:chunks) ./ chunks)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[39]:\n\n\n\n\n\n4-element Array{Float64,1}:\n 0.25\n 0.5 \n 0.75\n 1.0 \n\n\n\n\n\n\n\n\n\n\n\nOOOOOOOOOOOOOH. So ./ is a a lazy operator! In other words, if you do this:\n\n\n\n\n\n\nIn\u00a0[43]:\n\n    \nsqrt((1:chunks) ./ chunks)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\nMethodError: no method matching sqrt(::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}})\nClosest candidates are:\n  sqrt(!Matched::Float16) at math.jl:1018\n  sqrt(!Matched::Complex{Float16}) at math.jl:1019\n  sqrt(!Matched::Missing) at math.jl:1070\n  ...\n\nStacktrace:\n [1] top-level scope at In[43]:1\n\n\n\n\n\n\n\n\n\n\nThis errors because you're operating on a range, but instead if you do this:\n\n\n\n\n\n\nIn\u00a0[44]:\n\n    \nsqrt.((1:chunks) ./ chunks)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[44]:\n\n\n\n\n\n4-element Array{Float64,1}:\n 0.5               \n 0.7071067811865476\n 0.8660254037844386\n 1.0               \n\n\n\n\n\n\n\n\n\n\n\nChains of broadcasting operations \"materialize\" only once - skipping plenty of unnecessary overhead.\nThis is badass and I have to admit that I'm rather hyped up for Julia now!",
      "tags": "julia",
      "url": "https://stanczakdominik.github.io/posts/first-dive-julia/"
    },
    {
      "title": "Parsing and plotting LaTeX expressions with SymPy",
      "text": "Today let's look into some pretty neat SymPy functionality. I was in a fluid dynamics lecture, practicing taking notes with LaTeX on the go and stumbled upon this monstrosity:\n$$ \\Delta(k) = \\frac{\\rho_1-\\rho_2}{\\rho_1 + \\rho_2} gk + \\frac{\\gamma k3}{\\rho_1 + \\rho_2} - \\frac{\\rho_1 \\rho_2}{(\\rho_1 + \\rho_2)2} U2 k2 $$(bonus points for whoever recognizes this!)\nWe were supposed to draw this for a few example sets of values. All right! I opened up pinta and scribbled a few squiggly lines with my small touchpad, following the blackboard drawings. It looked darn ugly, but that got me thinking. SymPy has parsers, right? Can't I just parse that LaTeX equation into Python and make that plot pretty with matplotlib?\nWell, as it turns out, sure...\n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \nthe_plot.show()\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut it takes some tinkering.\n\nAll right, let the tinkering commence! Let's get straight to the point. For this to run, you'll need antlr4 (in current Jupyter, you can simply do %conda install antlr-python-runtime from within the Notebook).\nWe're going to simply dump the LaTeX string into sympy.parsing.latex.parse_latex, with the important caveat - this needs to be a r\"raw string\". Otherwise, LaTeX is going to go wild put a carriage return into every \\rho.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nimport sympy\nfrom sympy.parsing.latex import parse_latex\nlatex_string = r\"\\Delta(k) = \\frac{\\rho_1-\\rho_2}{\\rho_1 + \\rho_2} gk + \\frac{\\gamma k3}{\\rho_1 + \\rho_2} - \\frac{\\rho_1 \\rho_2}{(\\rho_1 + \\rho_2)2} U2 k2\"\nequation = parse_latex(latex_string)\nequation\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[2]:\n\n\n\n\n\nEq(Delta(k), -U**2*k**2*rho_{1}*rho_{2}/(rho_{1} + rho_{2})**2 + (g*k)*((rho_{1} - rho_{2})/(rho_{1} + rho_{2})) + (gamma*k**3)/(rho_{1} + rho_{2}))\n\n\n\n\n\n\n\n\n\n\n\nWe can access the variables we'd like to substitute (as SymPy symbols) using equation.free_symbols:\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \nequation.free_symbols\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[3]:\n\n\n\n\n\n{U, g, gamma, k, rho_{1}, rho_{2}}\n\n\n\n\n\n\n\n\n\n\n\nIdeally what I'd like to do is use .subs on the equation to plug in numerical values. To achieve this, it would probably be easiest to turn the symbols into Python variables. However...\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \nU, g, gamma, k, rho_1, rho_2 = equation.free_symbols\nU, gamma, rho_1\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[7]:\n\n\n\n\n\n(g, U, gamma)\n\n\n\n\n\n\n\n\n\n\n\n... the unordered nature of Python's set comes back with a vengeance! It's not too trivial to get these out in the right order. You could try sorted, but one does not simply compare Symbols:\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nsorted(equation.free_symbols)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-8-f67f3a401c32> in <module>\n----> 1 sorted(equation.free_symbols)\n\n/progs/miniconda3/lib/python3.7/site-packages/sympy/core/relational.py in __nonzero__(self)\n    227 \n    228     def __nonzero__(self):\n--> 229         raise TypeError(\"cannot determine truth value of Relational\")\n    230 \n    231     __bool__ = __nonzero__\n\nTypeError: cannot determine truth value of Relational\n\n\n\n\n\n\n\n\n\n\nWhat I ended up doing here is:\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nU, g, gamma, k, rho_1, rho_2 = sorted(equation.free_symbols,\n                                      key = lambda x: str(x)   # the literal key part here - just sort them alphabetically!\n                                     )\nU, g, gamma, k, rho_1, rho_2\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[9]:\n\n\n\n\n\n(U, g, gamma, k, rho_{1}, rho_{2})\n\n\n\n\n\n\n\n\n\n\n\nAnd now we can simply use subs with a dictionary:\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \nequation.subs(dict(rho_1=1,\n                   rho_2=2,\n                   gamma=1,\n                   g=1,\n                  )\n             )\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[10]:\n\n\n\n\n\nEq(Delta(k), -U**2*k**2*rho_{1}*rho_{2}/(rho_{1} + rho_{2})**2 + k**3/(rho_{1} + rho_{2}) + k*(rho_{1} - rho_{2})/(rho_{1} + rho_{2}))\n\n\n\n\n\n\n\n\n\n\n\n... or can we? This does not work on rho_{1} and rho_{2}. Here's why:\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \ndict(rho_1=1,\n     rho_2=2,\n     gamma=1,\n     g=1,\n)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[11]:\n\n\n\n\n\n{'rho_1': 1, 'rho_2': 2, 'gamma': 1, 'g': 1}\n\n\n\n\n\n\n\n\n\n\n\nWell duh, those are string values when input this way, and \"rho_1\" != \"rho_{1}\"!\nWe could instead do the following:\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \nbetter_dict = {rho_1: 1,\n rho_2: 2,\n gamma: 1,\n g: 1,\n}\nbetter_dict\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[12]:\n\n\n\n\n\n{rho_{1}: 1, rho_{2}: 2, gamma: 1, g: 1}\n\n\n\n\n\n\n\n\n\n\n\nWill that work?\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \nequation.subs(better_dict)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[13]:\n\n\n\n\n\nEq(Delta(k), -2*U**2*k**2/9 + k**3/3 - k/3)\n\n\n\n\n\n\n\n\n\n\n\nFinally! However, along the way you may have noticed a simpler way to do this:\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \nsimpler_equation = equation.subs({\n     \"rho_{1}\": 1,\n     \"rho_{2}\": 2,\n     \"gamma\": 1,\n     \"g\": 1,\n})\nsimpler_equation\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[14]:\n\n\n\n\n\nEq(Delta(k), -2*U**2*k**2/9 + k**3/3 - k/3)\n\n\n\n\n\n\n\n\n\n\n\nNote how this did not need us to even touch equation.free_symbols or mess around with sorted at all! I'm leaving the exploratory part here though - it might help someone looking to access variables in a parse_latex expression.\nWe may now plot it:\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \nDeltaK = simpler_equation.rhs\nDeltaK\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[15]:\n\n\n\n\n\n-2*U**2*k**2/9 + k**3/3 - k/3\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \nimport sympy.plotting\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = 12, 8\nk_range = (k, 0, 300)\ncolors = [\"blue\", \"green\", \"red\"]\nU_values = [1, 20, 50]\nplots = []\nfor u, color in zip(U_values, colors):\n    plot = sympy.plot(DeltaK.subs(U, u), k_range,\n                      show=False,\n                     line_color=color,\n                     legend=True,\n                     ylabel=r\"$\\Delta(k)$\",\n                     ylim = (-1e6, 1e6),\n                     xlim = (0, 300),\n                     title = f\"${latex_string}$\",\n                     )\n    plots.append(plot)\n\nplots[0].extend(plots[1])\nplots[0].extend(plots[2])\nthe_plot = plots[0]\nthe_plot.show()\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd, while not beautiful, it's much more pretty than what I got together with pinta!",
      "tags": "",
      "url": "https://stanczakdominik.github.io/posts/parsing-and-plotting-latex-expressions-with-sympy/"
    },
    {
      "title": "CuPy speedup of naive N-Body vectorized force calculation",
      "text": "I had intended to write a post about speeding up our Numpy Ising implementation, which we found out gave reasonable numerical values, though the small grids we were able to use limited the accuracy a fair bit. However, a few difficulties came up, so I thought instead (to keep writing these a habit!) I would write a little bit about using CuPy to speed up force calculations in N-body simulations. This might be a point I'll come back to later on this blog, as I have an ongoing project implementing that.\n\n\nThe part of the n-body simulation we'll look at is the calculation of forces, where the force on the i-th point particle or celestial object is:\n$$ F_i = \\sum_j F_{ij} = G \\sum_j \\frac{m_i m_j}{|\\vec{r_i}-\\vec{r_j}|3 } (\\vec{r_i}-\\vec{r_j}) $$From this, Newton's law gives $\\vec{a_i} = \\vec{F_i} / m_i$. I was trying to use my own implementation of this vectorization, but I found a neat implementation by PMende on Stack that's both more general and faster than what I had been doing. Let's take a look!\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport numpy\nimport cupy\nimport numpy_html\n\n\n    \n\n\n\n\n\n\n\nI basically copypasted the following:\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \ndef accelerations(positions, masses, G = 1):\n    '''\n    https://stackoverflow.com/a/52562874\n    \n    Params:\n    - positions: numpy array of size (n,3)\n    - masses: numpy array of size (n,)\n    '''\n    xp = cupy.get_array_module(positions)\n    mass_matrix = masses.reshape((1, -1, 1))*masses.reshape((-1, 1, 1))\n    disps = positions.reshape((1, -1, 3)) - positions.reshape((-1, 1, 3)) # displacements\n    dists = xp.linalg.norm(disps, axis=2)\n    dists[dists == 0] = 1 # Avoid divide by zero warnings\n    forces = G*disps*mass_matrix/xp.expand_dims(dists, 2)**3\n    return forces.sum(axis=1)/masses.reshape(-1, 1)\n\n\n    \n\n\n\n\n\n\n\nThe main change I made was adding this line:\nxp = cupy.get_array_module(positions)\n\nWhich returns numpy if we pass in a numpy.ndarray and cupy if we pass in a CuPy array. This will make this function more generic for our purposes.\nLet's take a look at what each of those lines does. For the illustrations, we'll take some particularly simple values:\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \nN = 5\nm = numpy.arange(N) + 1\nr = (numpy.arange(N*3)**2).reshape((N, 3))\nm\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[3]:\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\n\n\n\n\n\n\n\n\nPretty printing of arrays, by the way, is provided by the awesome numpy_html package. We can now start digging! Let's first investigate the mass_matrix:\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nmass_matrix = m.reshape((1, -1, 1)) * m.reshape((-1, 1, 1))\nmass_matrix.T\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[4]:\n\n\n\n\n\n\n\n\n1 \n2 \n3 \n4 \n5 \n\n\n2 \n4 \n6 \n8 \n10\n\n\n3 \n6 \n9 \n12\n15\n\n\n4 \n8 \n12\n16\n20\n\n\n5 \n10\n15\n20\n25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis was a (5, 5, 1)-shaped array, but I used a .T transposition so that it would print more nicely, as a (1, 5, 5)-shaped array. This shows that the (i, j)-th entry is just $m_i m_j$ - with the shape it has, we should be able to take advantage of Numpy broadcasting in our calculation.\nLet's now look at the displacements - disps. The line is\ndisps = r.reshape((1, -1, 3)) - r.reshape((-1, 1, 3))\n\nbut let's break it down a little bit more. r is\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \nr\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[5]:\n\n\n\n\n\n\n0  \n1  \n4  \n\n\n9  \n16 \n25 \n\n\n36 \n49 \n64 \n\n\n81 \n100\n121\n\n\n144\n169\n196\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhile if you reshape it with a single added dimension (signified by 1) in the first place:\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nr.reshape((1, -1, 3))\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[6]:\n\n\n\n\n\n\n\n\n0  \n1  \n4  \n\n\n9  \n16 \n25 \n\n\n36 \n49 \n64 \n\n\n81 \n100\n121\n\n\n144\n169\n196\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhile if we were to add a dimension in the second slot:\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \nr.reshape((-1, 1, 3))\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[7]:\n\n\n\n\n\n\n\n\n0  \n1  \n4  \n\n\n\n\n\n\n9  \n16 \n25 \n\n\n\n\n\n\n36 \n49 \n64 \n\n\n\n\n\n\n81 \n100\n121\n\n\n\n\n\n\n144\n169\n196\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe thus have a (1, 5, 3)-shaped array and a (5, 1, 3) array. Numpy (and anything implementing Numpy's broadcasting API by extension) is going to expand that into a (5, 5, 3) array:\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \ndisps = r.reshape((1, -1, 3)) - r.reshape((-1, 1, 3))\ndisps.T\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[8]:\n\n\n\n\n\n\n\n\n0   \n-9  \n-36 \n-81 \n-144\n\n\n9   \n0   \n-27 \n-72 \n-135\n\n\n36  \n27  \n0   \n-45 \n-108\n\n\n81  \n72  \n45  \n0   \n-63 \n\n\n144 \n135 \n108 \n63  \n0   \n\n\n\n\n\n\n0   \n-15 \n-48 \n-99 \n-168\n\n\n15  \n0   \n-33 \n-84 \n-153\n\n\n48  \n33  \n0   \n-51 \n-120\n\n\n99  \n84  \n51  \n0   \n-69 \n\n\n168 \n153 \n120 \n69  \n0   \n\n\n\n\n\n\n0   \n-21 \n-60 \n-117\n-192\n\n\n21  \n0   \n-39 \n-96 \n-171\n\n\n60  \n39  \n0   \n-57 \n-132\n\n\n117 \n96  \n57  \n0   \n-75 \n\n\n192 \n171 \n132 \n75  \n0   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich I chose to print with a transpose (as a (3, 5, 5) array) so as to illustrate the structure a bit more. Each of the three (5, 5) arrays displays a different spatial component of $\\vec{r_i} - \\vec{r_j}$. The arrays are antisymmetric as $\\vec{r_{ij}} = - \\vec{r_{ji}}$.\nLet's continue with the calculations! The next line simply calculates the norms of those inter-particle distances, outputting an (N, N) array (summing over the \"spatial dimensions\" axis):\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \ndists = np.linalg.norm(disps, axis=2)\ndists\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[9]:\n\n\n\n\n\n\n0.          \n27.33130074 \n84.85281374 \n173.35224256\n292.95733478\n\n\n27.33130074 \n0.          \n57.78408085 \n146.47866739\n266.22359024\n\n\n84.85281374 \n57.78408085 \n0.          \n88.74119675 \n208.53776636\n\n\n173.35224256\n146.47866739\n88.74119675 \n0.          \n119.81235329\n\n\n292.95733478\n266.22359024\n208.53776636\n119.81235329\n0.          \n\n\n\n\n\n\n\n\n\n\n\n\n\nThe next step is pretty clever. Since in disps (see above) each diagonal element is zero (since that's $\\vec{r_{ii}}$), and we'll be dividing those by distances, we're going to have Numpy screaming obscenities at us for dividing by zero. But since 0 / 1 = 0, we lose nothing and gain peace of mind by doing:\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \ndists[dists == 0] = 1\ndists\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[10]:\n\n\n\n\n\n\n1.          \n27.33130074 \n84.85281374 \n173.35224256\n292.95733478\n\n\n27.33130074 \n1.          \n57.78408085 \n146.47866739\n266.22359024\n\n\n84.85281374 \n57.78408085 \n1.          \n88.74119675 \n208.53776636\n\n\n173.35224256\n146.47866739\n88.74119675 \n1.          \n119.81235329\n\n\n292.95733478\n266.22359024\n208.53776636\n119.81235329\n1.          \n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple and effective! In my own implementation I had np.inf instead of 1 so as to get anything / np.inf == 0, but if anything = 0 for the problematic cases, that's fine as well.\nThe next line simply adds a dimension:\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \ndists.shape, np.expand_dims(dists, 2).shape\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[11]:\n\n\n\n\n\n((5, 5), (5, 5, 1))\n\n\n\n\n\n\n\n\n\n\n\nFor those curious as to why not just .reshape((-1, -1, 1)):\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \ntry:\n    dists.reshape((-1, -1, 1))\nexcept ValueError as e:\n    print(e)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ncan only specify one unknown dimension\n\n\n\n\n\n\n\n\n\n\n\nAnd now we can finally calculate the forces themselves, first getting each of $\\vec{F_{ij}}$:\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \nG = 1  # because let's be real...\nforces = G * disps * mass_matrix / np.expand_dims(dists, 2) ** 3\nforces\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[13]:\n\n\n\n\n\n\n\n\n0.             \n0.             \n0.             \n\n\n0.00088164     \n0.0014694      \n0.00205716     \n\n\n0.00017678     \n0.0002357      \n0.00029463     \n\n\n6.2195164e-05  \n7.60163116e-05 \n8.98374591e-05 \n\n\n2.86364625e-05 \n3.34092063e-05 \n3.81819501e-05 \n\n\n\n\n\n\n-0.00088164    \n-0.0014694     \n-0.00205716    \n\n\n0.             \n0.             \n0.             \n\n\n0.00083963     \n0.00102622     \n0.00121281     \n\n\n0.00018327     \n0.00021382     \n0.00024436     \n\n\n7.15474501e-05 \n8.10871102e-05 \n9.06267702e-05 \n\n\n\n\n\n\n-0.00017678    \n-0.0002357     \n-0.00029463    \n\n\n-0.00083963    \n-0.00102622    \n-0.00121281    \n\n\n0.             \n0.             \n0.             \n\n\n0.00077271     \n0.00087574     \n0.00097877     \n\n\n0.00017863     \n0.00019848     \n0.00021833     \n\n\n\n\n\n\n-6.2195164e-05 \n-7.60163116e-05\n-8.98374591e-05\n\n\n-0.00018327    \n-0.00021382    \n-0.00024436    \n\n\n-0.00077271    \n-0.00087574    \n-0.00097877    \n\n\n0.             \n0.             \n0.             \n\n\n0.0007326      \n0.00080237     \n0.00087214     \n\n\n\n\n\n\n-2.86364625e-05\n-3.34092063e-05\n-3.81819501e-05\n\n\n-7.15474501e-05\n-8.10871102e-05\n-9.06267702e-05\n\n\n-0.00017863    \n-0.00019848    \n-0.00021833    \n\n\n-0.0007326     \n-0.00080237    \n-0.00087214    \n\n\n0.             \n0.             \n0.             \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd we can contract that to $\\vec{F_i}$ by summing over the other-particle index:\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \nforces.sum(axis = 1)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[14]:\n\n\n\n\n\n\n0.00114925     \n0.00181453     \n0.00247981     \n\n\n0.00021281     \n-0.00014827    \n-0.00050936    \n\n\n-6.50662895e-05\n-0.0001877     \n-0.00031034    \n\n\n-0.00028558    \n-0.00036321    \n-0.00044083    \n\n\n-0.00101141    \n-0.00111535    \n-0.00121928    \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd from here, a simple division by the masses suffices to get the acceleration, but we need to remember to turn the mass into a (N, 1) array via a simple reshape:\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \nforces.sum(axis=1)/m.reshape(-1, 1)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[15]:\n\n\n\n\n\n\n0.00114925     \n0.00181453     \n0.00247981     \n\n\n0.00010641     \n-7.4137417e-05 \n-0.00025468    \n\n\n-2.16887632e-05\n-6.25669817e-05\n-0.00010345    \n\n\n-7.13957375e-05\n-9.08016861e-05\n-0.00011021    \n\n\n-0.00020228    \n-0.00022307    \n-0.00024386    \n\n\n\n\n\n\n\n\n\n\n\n\n\nLet's run a quick test first that also serves to illustrate the results.  We'll first write a simple function that prepares some reasonable parameters - masses and positions in arrays provided by our chosen packages.\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \ndef prep(N, np = numpy, seed = 0):\n    np.random.seed(seed)\n    m = np.abs(np.random.normal(loc=100, scale=20, size=N))\n    r = np.random.normal(size=(N, 3))\n    return r, m\n\n\n    \n\n\n\n\n\n\n\nFor a test, we'll set z = 0:\n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \nr, m = prep(10, numpy, seed = 17)\nr[:, -1] = 0\n\nax, ay, az = accelerations(r, m).T\nx, y, z = r.T\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.scatter(x, y, m);\nplt.quiver(x, y, ax, ay);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooks just about right! The system is evidently self-gravitating. Let's do the same for a few more bodies:\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \nr, m = prep(500, numpy, seed = 17)\nr[:, -1] = 0\n\nax, ay, az = accelerations(r, m).T\nx, y, z = r.T\n\nplt.scatter(x, y, m);\nplt.quiver(x, y, ax, ay);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd that's a proper mess, but the arrows seem to be oriented the right way (towards the system's center of mass) and you get a bunch of very long arrows, signifying high forces at short distances - another issue that I might well come back to in another post!\nAnd the nice thing is that our function works just as well on the GPU!\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \nax, ay, az = cupy.asnumpy(accelerations(cupy.asarray(r), cupy.asarray(m)).T)\n\nplt.scatter(x, y, m);\nplt.quiver(x, y, ax, ay);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet's now run a quick bechmark:\n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \nresults = []\nnumbers_of_bodies = [2**n for n in range(4, 13)]\nfor np in [numpy, cupy]:\n    for N in numbers_of_bodies:\n        r, m = prep(N, np, seed=17)\n        time = %timeit -oq accelerations(r, m)\n        results.append({\"library\":np.__name__,\n                        \"N\": N,\n                        \"average\": time.average,\n                        \"stdev\": time.stdev})\n\n\n    \n\n\n\n\n\n\nIn\u00a0[24]:\n\n    \nimport pandas\n\ndf = pandas.DataFrame(results)\ndf\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[24]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      N\n      average\n      library\n      stdev\n    \n  \n  \n    \n      0\n      16\n      0.000076\n      numpy\n      0.000003\n    \n    \n      1\n      32\n      0.000179\n      numpy\n      0.000002\n    \n    \n      2\n      64\n      0.000589\n      numpy\n      0.000020\n    \n    \n      3\n      128\n      0.002245\n      numpy\n      0.000139\n    \n    \n      4\n      256\n      0.012159\n      numpy\n      0.003883\n    \n    \n      5\n      512\n      0.044007\n      numpy\n      0.004827\n    \n    \n      6\n      1024\n      0.187316\n      numpy\n      0.007669\n    \n    \n      7\n      2048\n      0.718909\n      numpy\n      0.027526\n    \n    \n      8\n      4096\n      2.867943\n      numpy\n      0.055799\n    \n    \n      9\n      16\n      0.001288\n      cupy\n      0.000021\n    \n    \n      10\n      32\n      0.001316\n      cupy\n      0.000030\n    \n    \n      11\n      64\n      0.001463\n      cupy\n      0.000146\n    \n    \n      12\n      128\n      0.001430\n      cupy\n      0.000112\n    \n    \n      13\n      256\n      0.001321\n      cupy\n      0.000024\n    \n    \n      14\n      512\n      0.001656\n      cupy\n      0.000022\n    \n    \n      15\n      1024\n      0.005480\n      cupy\n      0.000082\n    \n    \n      16\n      2048\n      0.020252\n      cupy\n      0.000017\n    \n    \n      17\n      4096\n      0.080994\n      cupy\n      0.000289\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[25]:\n\n    \nfig, ax = plt.subplots()\nax.set_ylabel(\"Average runtime [s]\")\nfor label, g in df.groupby('library'):\n    g.plot('N', 'average', ax=ax, label=label, logx=True, logy=True, style=\"o--\")\nax.grid()\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThus at low numbers of particles, CuPy has a performance overhead, but at larger numbers of particles (as limited by MemoryErrors on my device, with the regime shifting around 100 particles), the GPU (predictably) wins!\nWe can also calculate the runtime ratios for a speedup estimate:\n\n\n\n\n\n\nIn\u00a0[26]:\n\n    \nspeedups = df[df.library =='numpy'].set_index('N').average / df[df.library =='cupy'].set_index('N').average\nspeedups\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[26]:\n\n\n\n\n\nN\n16       0.059079\n32       0.135752\n64       0.402321\n128      1.570570\n256      9.204320\n512     26.570801\n1024    34.179128\n2048    35.498837\n4096    35.409139\nName: average, dtype: float64\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[27]:\n\n    \nspeedups.plot(logx=True, style=\"o--\", logy=True)\nplt.ylabel(\"GPU over CPU speedup\");\nplt.grid()\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile this is obviously not a full proper test (we don't know how much host-device memory transfer would impact our timings, etc), it's at least nice to see that we get 35 times the speed on the GPU for the pure acceleration stage basically for free!",
      "tags": "cupy,gpu,nbody,python",
      "url": "https://stanczakdominik.github.io/posts/cupy-nbody-direct-force-calculation/"
    },
    {
      "title": "scipy.integrate.solve_ivp and makeshift Poincar\u00e9 sections of the Rossler attractor",
      "text": "I've just stumbled upon a relatively recent addition to scipy: integrate.solve_ivp, which looks amazing for the simulation of dynamical systems and solving equations where you'd like to detect discrete events occuring (say, collisions). Let's a look at what it's all about, and then use it to simulate the Rossler attractor!\n\n\n\n\n\n\nIn\u00a0[84]:\n\n    \nrossler_attractor\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[84]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs usual, we start with the imports:\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy.integrate import solve_ivp\nplt.rcParams['figure.figsize'] = [11, 6]  # for larger pictures\n\n\n    \n\n\n\n\n\n\n\nFirst let's remake an example from solve_ivp's documentation - an upward cannon shot, for which we'll want to get the trajectory from the moment of the launch until the cannonball hits the ground.\nWe define a derivative for our ODE (Newton's law with $g = -10 \\text{m}/\\text{s}2$ for simplicity):\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \ndef upward_cannon(t, y):\n        return [y[1],  # the derivative of the position is the velocity\n            -10    # the derivative of the velocity is the acceleration ~ -10 m/s2\n           ] \n\n\n    \n\n\n\n\n\n\n\nWe then define an event - a function that returns 0 once the event we're looking for has occured (in this case, once $y = 0$). This is pretty simple:\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \ndef hit_ground(t, y):\n    return y[0]\n\n\n    \n\n\n\n\n\n\n\nThis is where the cool part begins. We want to specify that the iteration should stop once the ground is hit, and we want to additionally specify that the function must go from positive to negative - otherwise, the start (from y = 0) would count! This is done, as per SciPy docs, via monkey patching and I won't deny I giggled when I figured this out:\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nhit_ground.terminal = True\nhit_ground.direction = -1\nhit_ground, hit_ground.terminal, hit_ground.direction\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[4]:\n\n\n\n\n\n(<function __main__.hit_ground(t, y)>, True, -1)\n\n\n\n\n\n\n\n\n\n\n\nLet's run the function now:\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \nsol = solve_ivp(upward_cannon,    # derivative function\n                [0, 10],          # time span for integration - make this generous\n                [0, 10],          # initial condition\n                events=hit_ground)\nsol\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[5]:\n\n\n\n\n\n  message: 'A termination event occurred.'\n     nfev: 38\n     njev: 0\n      nlu: 0\n      sol: None\n   status: 1\n  success: True\n        t: array([0.00000000e+00, 9.99900005e-05, 1.09989001e-03, 1.10988901e-02,\n       1.11088891e-01, 1.11098890e+00, 2.00000000e+00])\n t_events: [array([2.])]\n        y: array([[ 0.00000000e+00,  9.99850015e-04,  1.09928513e-02,\n         1.10372974e-01,  1.04918520e+00,  4.93840733e+00,\n         1.77635684e-15],\n       [ 1.00000000e+01,  9.99900010e+00,  9.98900110e+00,\n         9.88901110e+00,  8.88911109e+00, -1.10988896e+00,\n        -1.00000000e+01]])\n\n\n\n\n\n\n\n\n\n\n\nAll right, the cannonball's flight was successfully terminated.\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nplt.plot(sol.t, sol.y[0], label=\"vertical position\")\nplt.plot(sol.t, sol.y[1], label=\"vertical velocity\")\nplt.legend();\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGosh, ain't that plot ugly, though! We can fix it with the t_eval parameter to solve_ivp, which forces calculations of the function at the prescribed times. We'll also add another event that calculates when the cannonball stops in mid-air:\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \ndef zero_velocity(t, y):\n    return y[1]\n\nt = np.linspace(0, 10)\nsol_t_eval = solve_ivp(upward_cannon,\n                       [0, 10],\n                       [0, 10],\n                       events=[hit_ground, zero_velocity],\n                       t_eval=t)\nplt.plot(sol_t_eval.t, sol_t_eval.y[0])\nplt.plot(sol_t_eval.t, sol_t_eval.y[1])\nsol_t_eval.t_events\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[7]:\n\n\n\n\n\n[array([2.]), array([1.])]\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOf course, the first array of event times corresponds to hitting-the-ground-with-a-thud, and the second corresponds to stopping in midair. Notice how the trajectory seems to stop early, though:\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nsol_t_eval.t.max()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[8]:\n\n\n\n\n\n1.836734693877551\n\n\n\n\n\n\n\n\n\n\n\nBut that's just a case of having few (50 by default) points in our linspace:\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nsol_t_eval_fixed = solve_ivp(upward_cannon,\n                           [0, 10],\n                           [0, 10],\n                           events=[hit_ground, zero_velocity],\n                           t_eval=np.linspace(0, 10, 200))\nplt.plot(sol_t_eval_fixed.t, sol_t_eval_fixed.y[0])\nplt.plot(sol_t_eval_fixed.t, sol_t_eval_fixed.y[1])\nsol_t_eval_fixed.t.max()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[9]:\n\n\n\n\n\n1.9597989949748744\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMuch better! And now, let's abandon this toy example and do something I've been putting off for a good while, now:\nPoincar\u00e9 section of the Rossler attractor\u00b6As seen on Wikipedia (and it is amazing that if you go to edit, you can just copypaste the LaTeX source), the Rossler system is defined by the following set of differential equations:\n$$ \\begin{cases}  \\frac{dx}{dt} = -y - z \\\\ \\frac{dy}{dt} = x + ay \\\\ \\frac{dz}{dt} = b + z(x-c) \\end{cases} $$\nWe'll take sample parameters from Wikipedia and set up the derivative function:\n\n\n\n\n\n\nIn\u00a0[55]:\n\n    \na=0.1\nb=0.1\nc=14\nx_section = 10\n\ndef rossler(t, vector):\n    x, y, z = vector   # for readability\n    return [-y -z,\n            x + a * y,\n            b + z * (x - c),\n           ]\n\n\n    \n\n\n\n\n\n\n\nWhat we want to do here is figure out a way to take a Poincar\u00e9 (with whom I have a love-hate relationship, the love coming from his achievements and the hate coming from the \u00e9 I have had to paste multiple times here) section. The way I think about Poincar\u00e9 sections is that they're simply intersections of the trajectory of a system (the $\\vec{y}(t) = (x(t), y(t), z(t)$ curve) with some particular surface. Wikipedia has a more formal definition of those.\nWe'll define our Poincar\u00e9 section as $x = 10, \\dot{x} < 0$. I picked $x=10$ simply because the plots came out nicely that way.\nEDIT: THE NEXT PART HAS TURNED OUT TO BE WILDLY INCORRECT, PLEASE TAKE NOTE THAT WHAT I SAID HERE WAS WRONG. THE PROPER SOLUTION IS BELOW\nUnfortunately, solve_ivp at the time of writing does not seem to support saving values of the vector $\\vec{y} = (x, y, z)$ at event times - it simply saves the times themselves. That's probably a future PR - but for now, we can hack that ourselves, given that we're just interested in collecting a bunch of points at $x \\approx 10$:\n\n\n\n\n\n\nIn\u00a0[56]:\n\n    \nevents = []\ndef poincare(t, vector):\n    x = vector[0]\n    if np.isclose(x, x_section, rtol=1e-9, atol=1e-12):\n        events.append((t, vector))\n    return x - x_section\npoincare.direction = -1    # decreasing x\n\nsol = solve_ivp(rossler,\n               [0, 500000],\n               [-0.2, 0.2, 5.2],\n               events=poincare)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[57]:\n\n    \nlen(sol.t_events[0])/len(events)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[57]:\n\n\n\n\n\n0.4678223495702006\n\n\n\n\n\n\n\n\n\n\n\nTurns out we got about twice as many section events as we should have. This is reasonable - as the solver of the equation defined by poincare(x,y,z) = 0 equation creeps closer and closer to a point where it's exactly satisfied, poincare is evaluated multiple times - and events gets multiple entries from each actual P-section. No matter! For most of our plots, this is going to be fine.\nWe'll plot the whole trajectory we got in glorious 3D, with the Poincar\u00e9 section in orange:\n\n\n\n\n\n\nIn\u00a0[58]:\n\n    \nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot(sol.y[0], sol.y[1], sol.y[2], ',')\n\nvectors = np.array([e[1] for e in events])\nt = np.array([e[0] for e in events])\nx, y, z = vectors.T\nax.plot(x, y, z, \".\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"z\");\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe'll take another picture from a different angle to show that this is indeed the Poincar\u00e9 section we were looking for:\n\n\n\n\n\n\nIn\u00a0[59]:\n\n    \nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.view_init(30, -90)\nax.plot(sol.y[0], sol.y[1], sol.y[2], ',')\nax.plot(x, y, z, \".\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"z\");\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also plot simply $y(t_n)$ for each passage through the $x = 10$ plane (in the correct direction):\n\n\n\n\n\n\nIn\u00a0[60]:\n\n    \nplt.plot(t, y, \",\")\nplt.xlabel(\"t\")\nplt.ylabel(\"y\");\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich is of course rather unenlightening. What we would be ultimately interested in is a Poincar\u00e9 map - a map of $y(t_{n+1})$ plotted against $y(t_n)$. This has many useful properties that I'm just beginning to learn about. Let's see what we get:\n\n\n\n\n\n\nIn\u00a0[61]:\n\n    \nplt.plot(y[:-1], y[1:], \",\")\nplt.xlabel(r\"$y(t_n)$\")\nplt.ylabel(r\"$y(t_{n+1})$\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[61]:\n\n\n\n\n\nText(0, 0.5, '$y(t_{n+1})$')\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that \",\" passed to plt.plot draws a single pixel at each point's location. If you look closely, you can see something like a 3D-rotated peak of the Rossler attractor (it's sort of shaped like a U, except the open-ended lines are bent towards the left).\nThere is also a line corresponding to $y(t_n) = y(t_{n+1})$. This one is to be expected as an artifact of our faulty point-gathering method. Remember that we gathered about twice as many points as there were points located by solve_ivp? Well, this just shows that those twice-as-many-points are mostly pairs of adjacent points close to each other! To illustrate what I mean, let me calculate $|y(t_{n+1}) - y(t_n)|$ for each $t_n$ in the trajectory and plot a histogram of that:\n\n\n\n\n\n\nIn\u00a0[62]:\n\n    \nplt.hist(np.abs(np.diff(y)), bins=100)\nplt.xlabel(r\"|y(t_n) = y(t_{n+1})|\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[62]:\n\n\n\n\n\nText(0.5, 0, '|y(t_n) = y(t_{n+1})|')\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat's about 100k points that didn't move much at all between two hits of the $x=10$ plane! That's clearly incorrect.\nFixed hack-less version\u00b6As MatthewFlamm points out on GitHub, though the documentation I had needed to see was only available on scipy's master branch, the dense_output flag to solve_ivp solves our issue:\n\n\n\n\n\n\nIn\u00a0[65]:\n\n    \ndef poincare(t, vector):\n    x = vector[0]\n    return x - x_section\n\npoincare.direction = -1    # decreasing x\nsol = solve_ivp(rossler,\n               [0, 500000],\n               [-0.2, 0.2, 5.2],\n               events=poincare,\n               dense_output=True)\n\nsol.sol\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[65]:\n\n\n\n\n\n<scipy.integrate._ivp.common.OdeSolution at 0x7f1a0d35cb38>\n\n\n\n\n\n\n\n\n\n\n\nThis returns a callable that we can use to interpolate the solution to any time point we choose, especially the event times:\n\n\n\n\n\n\nIn\u00a0[66]:\n\n    \nt = sol.t_events[0]\nvectors = sol.sol(t)\nvectors\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[66]:\n\n\n\n\n\narray([[10.        , 10.        , 10.        , ..., 10.        ,\n        10.        , 10.        ],\n       [ 6.43880307, 12.76136452,  8.36333749, ...,  7.63756891,\n         9.95585964, 13.35530381],\n       [ 0.03343946,  0.23215879, 19.95229153, ..., 23.0356289 ,\n         0.06066276,  3.4934677 ]])\n\n\n\n\n\n\n\n\n\n\n\nAnd now we just redo the plots we had before, making them a little nicer:\n\n\n\n\n\n\nIn\u00a0[83]:\n\n    \nrossler_attractor = plt.figure(figsize=(12, 8))\nax = rossler_attractor.add_subplot(111, projection='3d')\nax.plot(sol.y[0], sol.y[1], sol.y[2], ',', alpha=0.02)\n\nx, y, z = vectors\nax.plot(x, y, z, \".\", alpha=0.02)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"z\");\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the other angle:\n\n\n\n\n\n\nIn\u00a0[51]:\n\n    \nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.view_init(30, -90)\nax.plot(sol.y[0], sol.y[1], sol.y[2], ',')\nax.plot(x, y, z, \".\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"z\");\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[52]:\n\n    \nplt.plot(t, y, \",\")\nplt.xlabel(\"t\")\nplt.ylabel(\"y\");\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$y(t_{n+1})$ plotted against $y(t_n)$:\n\n\n\n\n\n\nIn\u00a0[53]:\n\n    \nplt.plot(y[:-1], y[1:], \",\")\nplt.xlabel(r\"$y(t_n)$\")\nplt.ylabel(r\"$y(t_{n+1})$\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[53]:\n\n\n\n\n\nText(0, 0.5, '$y(t_{n+1})$')\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, that actually looks like it should, the histogram below should also not have a peak at 0:\n\n\n\n\n\n\nIn\u00a0[54]:\n\n    \nplt.hist(np.abs(np.diff(y)), bins=100)\nplt.xlabel(r\"|y(t_n) = y(t_{n+1})|\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[54]:\n\n\n\n\n\nText(0.5, 0, '|y(t_n) = y(t_{n+1})|')\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd it does not, indeed!\nThus, solve_ivp becomes even more awesome than I had thought it was! Check it out!",
      "tags": "nonlinear-dynamics,python,rossler,scipy,simulation",
      "url": "https://stanczakdominik.github.io/posts/scipyintegratesolve_ivp-and-makeshift-poincare-sections-of-the-rossler-attractor/"
    },
    {
      "title": "Credits: people I wouldn't have gotten here without",
      "text": "In alphabetic order, as I would rather not judge their relative importances. If\nthe order is unalphabetic or if you think I have forgotten someone, \nPRs are welcome and angry IMs are as well, if you're into that kind of thing.\n\nKabat \"Piotr\" Kabaci\u0144ski, my partner in physics crime and StarCraft misdemeanor from the first years of university. Living proof that reaching out to people with interesting profile pictures (\"oh, isn't that Tassadar?\") is the best investment you can make in life.\nKarolina Czerniak, who's my personal definition of a scientific success story and a constant source of cheer in my life.\nMonika Seniut, who worked with me despite my quirks and always pushed me to improve further and further. Some say she's still out there, continuing to improve further and further to this day.\nPaulina Marikin, my wonderful girlfriend, who's always amazingly supportive, frankly amazing and will always provide you with an interesting fact about graph theory.\nPlasmaPy Community, The: for many a PR collaborated on and many a glorious pun made. I learned a lot from them.\nPythonistas, the people behind the entire open source and scientific computing ecosystem who give us all jobs - and not just those behind Python, of course!\nRichard Feynman, who needs no introduction and whose books basically got me into physics.\n\n... and many others that I'm sure I missed - my sincerest apologies!",
      "tags": "",
      "url": "https://stanczakdominik.github.io/credits/"
    },
    {
      "title": "No new post today...",
      "text": "Exams are a pain.",
      "tags": "",
      "url": "https://stanczakdominik.github.io/posts/no-new-post-today/"
    },
    {
      "title": "Quantitative data analysis of the 2D Ising model",
      "text": "Last time, we made a neat little implementation of a 2D Ising model simulation, and we showed that it looks reasonable. Well, that it might, but we can't be certain of that! I know I said that next time we would, er, %timeit, put it on the GPU and make it GO FAST, but perhaps it's a better idea to start with some data analysis first, making sure the result we're getting are quantitatively what we would like them to be - physical.\n\n\nOne more errata: conveniently forgetting to mention that we were working with the assumption that there's no external magnetic field. Let it be known that we're neglecting any external magnetic field. Let's resummarize the results here:\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline\n\n\n    \n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nplt.rcParams['figure.figsize'] = [11, 6]  # for larger pictures\nsize = (64, 64)\n\nnp.random.seed(0)\na = np.random.randint(low=0, high=2, size=size) * 2 - 1\nplt.imshow(a, cmap='binary')\nplt.colorbar();\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI'll modify the better_iteration function we arrived at in two ways: the name could be better, as in, it tries to flip half the spins on the grid, so we'll call it half_iteration. We'll also make it so that the spin matrix a is modified in-place, without copying, to save memory and computation time:\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \ndef half_iteration(a, mask, J = 1, beta = 1):\n    interactions = np.roll(a, 1, 0) + np.roll(a, 1, 1) + np.roll(a, -1, 0) + np.roll(a, -1, 1)\n    deltaE = 2 * J * a * interactions\n    boltzmann = np.exp(-beta * deltaE) * mask\n    flip_these = np.random.random(a.shape) < boltzmann\n    a[flip_these] *= -1\n\n# make a checkerboard pattern to preserve causality, updating spins on alternating diagonals in each iteration\ndef mask(a):\n    a_mask = np.ones_like(a)\n    a_mask[::2, ::2] = 0\n    a_mask[1::2, 1::2] = 0\n    return a_mask\n\ndef full_iteration(a, mask, J = 1, beta = 1):\n    # this now becomes a VERY thin wrapper!\n    half_iteration(a, mask, J, beta)\n    half_iteration(a, 1-mask, J, beta)\n\n\na_mask = mask(a)\nfull_iteration(a, a_mask)\nplt.imshow(a, cmap='binary')\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[3]:\n\n\n\n\n\n<matplotlib.image.AxesImage at 0x7f7e8556edd8>\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are now in the exact spot where we left off a week ago. Let's do some quantitative calculation now!\nThere are three main diagnostics (that I know of) that you would be interested in when looking at this kind of system:\n\nthe average magnetization, which is simply the average of all spins\nthe internal energy, which is -0.5 * deltaE in our half_iteration code for each spin (and globally, we would take the average of that, so technically it's the average internal energy per spin)\nthe heat capacity, which is the partial derivative of internal energy with respect to the temperature\n\nThe first two can be grabbed straightforwardly from each snapshot of the iteration, and then averaged to decrease the effect of fluctuations. The third, as seen on Richard Fitzpatrick's website, can be calculated as\n$$C_v = \\frac{dU}{dT} = \\frac{\\sigma_U2}{k_B T2} $$Let's get to it!\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \ndef magnetization(a):\n    return a.mean(), a.std()\n\ndef internal_energy(a, J = 1):\n    interactions = np.roll(a, 1, 0) + np.roll(a, 1, 1) + np.roll(a, -1, 0) + np.roll(a, -1, 1)\n    current_energy = -J * a * interactions\n    return current_energy.mean(), current_energy.std()\n\nmagnetization(a), internal_energy(a)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[4]:\n\n\n\n\n\n((0.0205078125, 0.9997896926986519), (-2.123046875, 1.816938556075228))\n\n\n\n\n\n\n\n\n\n\n\nWell, that's a whole lot of variance! Our system has definitely not converged to any stable state yet. Let's repeat that after a bunch of iterations:\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \na_bunch = 100\nfor i in range(a_bunch):\n    full_iteration(a, a_mask)\n\nplt.imshow(a, cmap='binary')\nmagnetization(a), internal_energy(a)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[5]:\n\n\n\n\n\n((0.9990234375, 0.04418338291352976), (-3.9921875, 0.19748788530882092))\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWell, that's better. A little bleak, but it certainly seems stable.\n\n\n\n\n\n\n\nAll right! This is what we wanted. The system has nearly converged to a stable state, so its magnetization (remember that we're way under the critical temperature) is almost fully in one direction, and the internal energy is way lower than what we've had before, suggesting that this state is much closer to equilibrium.\nLet's try to get the energies and plot this:\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nk_b = 1\nT_range = np.linspace(1.5, 3.5, 300)\niterations = 100\nenergies = []\nmagnetizations = []\nfor T in tqdm_notebook(T_range):\n    beta = 1 / (k_b * T)\n    np.random.seed(0)\n    a = np.random.randint(low=0, high=2, size=size) * 2 - 1\n    a_mask = mask(a)\n    for i in range(iterations):\n        full_iteration(a, a_mask, beta=beta)\n    energies.append(internal_energy(a))    \n    magnetizations.append(magnetization(a))   \n\nE, dE = np.array(energies).T\nM, dM = np.array(magnetizations).T\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#6fc2b443-aa59-4812-9a7e-574b83c59399');\n\n\n{\"model_id\": \"a40e4bd596bc4732a23cd2f8c218b885\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe'll also need a bunch of plots, and we'd like to use $C_v$ to let us find the critical temperature as exhibited by our simulation as its maximum. Let's stick the analysis we want into a function:\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \nOnsager_critical_temperature = 2 / np.log(1 + 2**0.5)  # a theoretical value\n\ndef analysis(T_range, E, dE, M, dM, plotting=True):\n    Cv = dE**2 / k_b / T_range**2  # see Fitzpatrick\n    maximum_index = np.argmax(Cv)\n    our_critical_temperature = T_range[maximum_index]\n    if plotting:\n        plt.plot(T_range, E)\n        plt.xlabel(r\"Temperature [in units where $k_B = 1$]\")\n        plt.ylabel(\"Average energy per spin\")\n        plt.xlim(T_range.min(), T_range.max());\n        plt.vlines(Onsager_critical_temperature, E.min(), E.max())\n\n        plt.figure()\n\n        plt.plot(T_range, M)\n        plt.xlabel(r\"Temperature [in units where $k_B = 1$]\")\n        plt.ylabel(\"Average magnetization\")\n        plt.xlim(T_range.min(), T_range.max());\n        plt.vlines(Onsager_critical_temperature, M.min(), M.max())\n\n\n        plt.figure()\n        plt.plot(T_range, Cv, \"o-\")\n        plt.xlabel(r\"Temperature [in units where $k_B = 1$]\")\n        plt.ylabel(\"Heat capacity per spin\")\n        plt.xlim(T_range.min(), T_range.max());\n        plt.vlines(Onsager_critical_temperature, Cv.min(), Cv.max())\n\n    return our_critical_temperature\nanalysis(T_range, E, dE, M, dM)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[7]:\n\n\n\n\n\n2.2959866220735785\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd if I've ever seen a plot that says nothing, it's this one. It seems all random. There's a simple issue underneath this: since we're starting from a randomized grid, there is no telling which spin state the system will converge to at a low temperature. Note how there's much less noise above the critical temperature as found by Onsager back in 1944, as denoted by the vertical line. This makes intuitive sense: above the critical temperature the system converges to an essentially random state, and each of those is basically equivalent.\nLet's try this again, from a cold start (all spins up):\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nenergies = []\nmagnetizations = []\nfor T in tqdm_notebook(T_range):\n    beta = 1 / (k_b * T)\n    a = np.ones(size)\n    a_mask = mask(a)\n    for i in range(iterations):\n        full_iteration(a, a_mask, beta=beta)\n    energies.append(internal_energy(a))    \n    magnetizations.append(magnetization(a))   \n\nE, dE = np.array(energies).T\nM, dM = np.array(magnetizations).T\n\nanalysis(T_range, E, dE, M, dM)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#afea5f2e-8206-4f10-baa7-36619561655f');\n\n\n{\"model_id\": \"62f734d7f5ea406ab12dd975c6568f37\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n    Out[8]:\n\n\n\n\n\n2.322742474916388\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat's a bit better, but not quite enough. The measurement  is still noisy. What we need to do is average the energies and magnetizations over a bunch of iterations. We'll also stick our entire logic into a function:\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \ndef simulation(iterations, size=size, T_range=T_range, k_b=k_b, plotting=True):\n    energies = []\n    magnetizations = []\n    a_mask = np.ones(size)\n    a_mask[::2, ::2] = 0\n    a_mask[1::2, 1::2] = 0\n    \n    for T in tqdm_notebook(T_range):\n        beta = 1 / (k_b * T)\n        a = np.ones(size)\n        E = np.zeros(2)\n        M = np.zeros(2)\n        for i in range(iterations):\n            E += internal_energy(a)\n            M += magnetization(a)\n            full_iteration(a, a_mask, beta=beta)\n        energies.append(E / iterations)\n        magnetizations.append(M / iterations)   \n\n    E, dE = np.array(energies).T\n    M, dM = np.array(magnetizations).T\n    return analysis(T_range, E, dE, M, dM, plotting)\nsimulation(1000)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#70fd46cd-a983-41b5-8c12-660bee3a0793');\n\n\n{\"model_id\": \"8de3c70e6715462891034757c3d41704\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n    Out[9]:\n\n\n\n\n\n2.32943143812709\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo we're getting relatively good agreement with Fitzpatrick's results, but on the other hand... our critical temperature is slightly off and the peak is not as sharp as it should probably be. Perhaps this is an issue of small grid size?\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \nsimulation(1000, (128, 128))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#9fdd38e0-5baa-4763-bea7-6f45ac715951');\n\n\n{\"model_id\": \"00f8dd2b103d491f8435d6fc91e22946\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n    Out[10]:\n\n\n\n\n\n2.3361204013377925\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOkay, and what if we go in the other direction, towards smaller sizes:\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \nsimulation(500, (16, 16))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#61c3681b-22f9-4573-b5f8-a8c715bb3a8c');\n\n\n{\"model_id\": \"2bc0a80235b84a2c9514885157417eae\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n    Out[11]:\n\n\n\n\n\n2.3896321070234112\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterestingly, while the peak's behavior is not changing, the estimated critical temperature does change a bit. However, I'm trying to write this blog in half-hour chunks, and running multiple simulations does eat into that period. In other words, while we have shown that the magnetization and internal energy qualitatively behave just as they should for these few runs, it'd be nice to find out whether we converge to the critical temperature - and we're going to need to run a whole bunch of simulations to do that. We do need to speed this up, after all!",
      "tags": "ising,numpy,python,simulation",
      "url": "https://stanczakdominik.github.io/posts/quantitative-data-analysis-of-the-2d-ising-model/"
    },
    {
      "title": "Parallelizable Numpy implementation of 2D Ising model",
      "text": "In the next few posts I'd like to discuss a fun project I've been procrastinating things with recently - a parallelizable (up to the GPU level) Python Numpy implementation of the 2D Ising model, which I won't introduce at length here because it's been covered really well in multiple places out there and I'd rather not repeat them.\nThe gist of it is that we have a grid of discrete spins represented by integers, +1 for spin up and -1 for spin down. Each spin interacts with its nearest neighbors via a sum of products of that spin's value and the neighbor's value, times minus a positive constant J. The minus is there so that spins pointing the same way decrease the total energy and spins pointing in the opposite direction increase it.\n\nLet's get to it!\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n    \n\n\n\n\n\n\n\nFor now, we'll run a 16x16 grid, representing spin up as black and spin down as white.\n\n\n\n\n\n\nIn\u00a0[25]:\n\n    \nplt.rcParams['figure.figsize'] = [11, 6]  # for larger pictures\nplt.rcParams['image.cmap'] = 'plasma'    # well duh\nsize = (16, 16)\n\nnp.random.seed(20)\na = np.random.randint(low=0, high=2, size=size) * 2 - 1\nplt.imshow(a, cmap='binary')\nplt.colorbar();\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe usual Monte Carlo Markov Chain (Metropolis algorithm) implementation for this goes like this:\n\npick a spin (point on the grid) at random\ncalculate the change in energy $\\Delta E$ that would result from flipping it\nuse that change in energy to figure out how likely the flipped state is via a Boltzmann $\\exp(\\beta \\Delta E)$ factor (clearly non-negative), calculated from interactions with neighboring spins\nuse that probability with an uniformly-distributed random number to reject or accept the state with the flipped state\nrun a boatload of iterations\n\nIt's a nice algorithm, don't get me wrong - but it seems needlessly complex to parallelize if you're just picking one spin at a time. Why not try to flip every spin at a time? Let's try that!\nFirst up, for each spin, we'll calculate its interaction terms, with periodic boundary conditions. This just means that for each spin, we add up the sum of its neighbors' spins (check the math, it really works like that), reaching to the other side on the edges. numpy.roll is amazing for that:\n\n\n\n\n\n\nIn\u00a0[26]:\n\n    \ninteractions = np.roll(a, 1, 0) + np.roll(a, 1, 1) + np.roll(a, -1, 0) + np.roll(a, -1, 1)\nplt.imshow(interactions)\nplt.colorbar();\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe'll just keep our constants \"reasonable\" (equal to 1), as simulationistas are wont to do. We'll keep writing them though, so as to keep in mind where they should go in. The full energy is just the local spin's value times its neighbors' spins (the local spin factors out of the sum), times a constant:\n\n\n\n\n\n\nIn\u00a0[28]:\n\n    \nJ = 1 # a material constant defining the strength of the interaction\n\ncurrent_energy = - J * a * interactions\nplt.imshow(current_energy)\nplt.colorbar();\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe'd expect the low energy spins to remain as they are, and the high energy spins to flip, so as to decrease the system's energy.\nIt's also simple to calculate the change in energy: boldly keeping the interaction terms constant, we just change the sign on a, so:\n\n\n\n\n\n\nIn\u00a0[29]:\n\n    \ndeltaE = 2 * J * a * interactions # = final state - initial state = -J interactions * ( (-a) - (a) )\nplt.imshow(deltaE)\nplt.colorbar();\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet's now get the Boltzmann factor:\n\n\n\n\n\n\nIn\u00a0[30]:\n\n    \nbeta = 1 # inverse thermal energy 1 / (k_B*T)\nboltzmann = np.exp(-beta * deltaE)\nplt.imshow(boltzmann)\nplt.colorbar();\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOuch, look at that colorbar! Note that we're only comparing the Boltzmann factor (which is in the range of $[0, \\infty)$ ) with a standard (phew, almost callled it a normal) random number with a uniform distribution from the $(0, 1)$ range. I'm not going to bother paying attention to the boundaries because they're infinitely unlikely for floats anyway.\nThus, a \"better\" (though admittedly uglier) plot that shows spins that are definitely going to flip in yellow is:\n\n\n\n\n\n\nIn\u00a0[32]:\n\n    \nplt.imshow(boltzmann, vmax = 1)\nplt.colorbar();\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGiven how ugly this plot was, let's take a quick look at the distribution of these Boltzmann factors, inverstigating their arguments first:\n\n\n\n\n\n\nIn\u00a0[33]:\n\n    \nnp.unique(-beta * deltaE)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[33]:\n\n\n\n\n\narray([-8, -4,  0,  4,  8])\n\n\n\n\n\n\n\n\n\n\n\nAnd now the actual Boltzmann factors, though we'll just force each one above 1 to be 1 exactly:\n\n\n\n\n\n\nIn\u00a0[34]:\n\n    \nnp.unique(boltzmann)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[34]:\n\n\n\n\n\narray([3.35462628e-04, 1.83156389e-02, 1.00000000e+00, 5.45981500e+01,\n       2.98095799e+03])\n\n\n\n\n\n\n\n\n\n\n\nIn other words, because of the discrete and simple ($s = \\pm1$) nature of the system, there are only a few possiblities.\nWe still need to figure out which spins will actually be flipped. That's quite simple as well.\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \nrandoms = np.random.random(size)\nflip_these = randoms < boltzmann\n\n_, axes = plt.subplots(ncols=2)\naxes[0].imshow(randoms);\naxes[1].imshow(flip_these.astype(int), cmap='binary');\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that we can now show the difference between which spins are definitely going to be flipped, and which spins are just \"randomly\" flipped:\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \nflip_these_for_sure = boltzmann >= 1\n_, axes = plt.subplots(ncols=2)\naxes[0].imshow(flip_these  flip_these_for_sure)\naxes[1].imshow(boltzmann, vmax = 1);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt's not many, but it's important to take those into account! And now we flip all of them, with a \"before vs after\" picture:\n\n\n\n\n\n\nIn\u00a0[35]:\n\n    \na_new = a.copy() # I'm keeping `a` for plotting, but you could certainly do it in-place\n\na_new[flip_these] *= -1\n\n_, axes = plt.subplots(ncols=2)\naxes[0].imshow(a, cmap='binary')\naxes[1].imshow(a_new, cmap = 'binary');\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDid that work? Well... not quite, as you may already see. We made a little assumption in there that is going to mess this up. Let me just resummarize the computational loop really quickly, and check that I did so correctly. Note how the actual code is pretty short, too.\n\n\n\n\n\n\nIn\u00a0[37]:\n\n    \ndef iteration(a, J = 1, beta = 1):\n    interactions = np.roll(a, 1, 0) + np.roll(a, 1, 1) + np.roll(a, -1, 0) + np.roll(a, -1, 1)\n    deltaE = 2 * J * a * interactions\n    boltzmann = np.exp(-beta * deltaE)\n    flip_these = np.random.random(a.shape) < boltzmann\n    new_a = a.copy()     # this could be neglected for an in-place implementation\n    new_a[flip_these] *= -1\n    return new_a\n\n_, axes = plt.subplots(ncols=2)\naxes[0].imshow(a, cmap='binary')\naxes[1].imshow(iteration(a), cmap = 'binary');\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo you see the problem yet? If not, this Python translation of a popular song should do it:\n\n\n\n\n\n\nIn\u00a0[38]:\n\n    \nold_boss = np.array([[1, -1],       # excuse my variable naming\n                    [-1, 1]])       # this is all part of the plan\n\n_, axes = plt.subplots(ncols=3)\naxes[0].imshow(old_boss, cmap='binary')\n\nboss = iteration(old_boss)\naxes[1].imshow(boss, cmap = 'binary');   # oh no...\n\nnew_boss = iteration(boss)\naxes[2].imshow(new_boss, cmap = 'binary');  # oh, yeaaah\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd if you haven't guessed yet:\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"zYMD_W_r3Fg\", start=472)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[15]:\n\n\n\n\n\n        \n        \n\n\n\n\n\n\n\n\n\n\n\nNote how I am merciful and spared you from having to lower your volume beforehand! You can trust me with these things.\nRemember that earlier we boldly (ctrl-f that if you don't recall!) kept the interaction terms constant. But that's a terrible mistake in terms of causality! Let's take a look at this again:\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \nplt.imshow(new_boss, cmap='binary')\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[16]:\n\n\n\n\n\n<matplotlib.image.AxesImage at 0x7f77dbb2d2b0>\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe black spins depend on the white spins for their energy calculation and vice versa. You can't update the white spins at the same time as the black ones are changing.\nThis pattern, though, provides a way out of this conundrum. We can use old_boss as a mask! We'll take turns updating the black and white spins this way.\n\n\n\n\n\n\nIn\u00a0[39]:\n\n    \ndef better_iteration(a, mask, J = 1, beta = 1):\n    interactions = np.roll(a, 1, 0) + np.roll(a, 1, 1) + np.roll(a, -1, 0) + np.roll(a, -1, 1)\n    deltaE = 2 * J * a * interactions # = final state - initial state = interactions * ( (-a) - (a) )\n    boltzmann = np.exp(-beta * deltaE) * mask    # this has been modified!\n    flip_these = np.random.random(a.shape) < boltzmann\n    new_a = a.copy()\n    new_a[flip_these] *= -1\n    return new_a\n\n_, axes = plt.subplots(ncols=3)\naxes[0].imshow(old_boss, cmap='binary')\n\nbetter_boss = better_iteration(old_boss, mask=old_boss)\naxes[1].imshow(better_boss, cmap = 'binary');\n\nbetter_new_boss = better_iteration(better_boss, mask= 1 - old_boss) # note the negation here\naxes[2].imshow(better_boss, cmap = 'binary');\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd to demonstrate that this doesn't get us spurious updates on the full grid:\n\n\n\n\n\n\nIn\u00a0[40]:\n\n    \na_mask = np.ones_like(a)\na_mask[::2, ::2] = 0\na_mask[1::2, 1::2] = 0\n\ndef full_iteration(a, mask, J = 1, beta = 1):\n    intermediate = better_iteration(a, mask, J, beta)\n    return better_iteration(intermediate, 1-mask, J, beta)\n\n_, axes = plt.subplots(ncols=4)\naxes[0].imshow(a_mask, cmap='binary');\naxes[1].imshow(a, cmap='binary')\naxes[2].imshow(better_iteration(a, a_mask), cmap='binary')\n\nfinal_a = full_iteration(a, a_mask)\naxes[3].imshow(final_a, cmap = 'binary');\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd I think we've earned this:\n\n\n\n\n\n\nIn\u00a0[19]:\n\n    \nYouTubeVideo(\"zYMD_W_r3Fg\", start=462) # headphone warning!\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[19]:\n\n\n\n\n\n        \n        \n\n\n\n\n\n\n\n\n\n\n\nLet's do one last check:\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \na_iterated = a.copy()\nnp.random.seed(0)\nfor i in range(10):\n    a_iterated = full_iteration(a_iterated, a_mask)\nplt.imshow(a_iterated, cmap='binary');\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWell, that's certainly physical behavior, though annoying - stable domains are a hallmark of the 2D Ising model under the critical temperature of about 2.3 (with $k_b = 1$). At high temperatures, the system behaves more randomly:\n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \nbeta = 1 / (1 * 10)\na_iterated = a.copy()\nnp.random.seed(0)\nfor i in range(10):\n    a_iterated = full_iteration(a_iterated, a_mask, beta = beta)\nplt.imshow(a_iterated, cmap='binary');\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile under the critical temperature, the long-term stable state is the single domain, all-spins-parallel state:\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \na_iterated = a.copy()\nnp.random.seed(0)\nfor i in range(1000):\n    a_iterated = full_iteration(a_iterated, a_mask)\nplt.imshow(a_iterated, cmap='binary');\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd that beautiful plot is probably a good place to finish for today! Next up, benchmarking and optimization (as in, just straight up dumping this on the GPU)!",
      "tags": "ising,numpy,python,simulation",
      "url": "https://stanczakdominik.github.io/posts/parallelizable-numpy-implementation-of-2d-ising-model/"
    },
    {
      "title": "NumPy-ish GPU computations with CuPy",
      "text": "I've recently come across the amazing CuPy library, and given that I haven't updated this blog in a while, I figured this would be a great opportunity to showcase a few of its capabilities.\nIf you haven't heard yet, CuPy is NumPy, but on the GPU, and it's amazing how close that simple description is to reality.\n\nFirst things first! Make sure you've installed it (I used Conda with Python 3.6) and that your Nvidia drivers are on. On my laptop, running an integrated Intel and dedicated Nvidia GPU, I had to simply run sudo modprobe nvidia.  Let's see if that worked:\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \n!nvidia-smi\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nTue Jan 22 08:08:35 2019       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 415.27       Driver Version: 415.27       CUDA Version: 10.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 106...  Off  | 00000000:01:00.0 Off |                  N/A |\n| N/A   65C    P0    24W /  N/A |      0MiB /  6078MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n\n\n\n\n\n\n\n\n\nYup! Let's get to it. We'll compare it with NumPy, of course:\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nimport cupy as cp\nimport numpy as np\n\n\n    \n\n\n\n\n\n\n\nI'm mostly interested in operations on dense matrices, so let's get ourselves a sample one. I'm not using an insanely large array due to MemoryErrors, but 2**20 floats should be a reasonable benchmark.\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \nN = 1024\nA = np.random.random((N, N))\nA\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[3]:\n\n\n\n\n\narray([[0.10388936, 0.27674225, 0.09349157, ..., 0.59858586, 0.01545899,\n        0.20201765],\n       [0.81588711, 0.19722361, 0.66885061, ..., 0.83687175, 0.15600763,\n        0.6171922 ],\n       [0.73374963, 0.66466975, 0.55082473, ..., 0.68605053, 0.93384799,\n        0.84729118],\n       ...,\n       [0.76718438, 0.40130284, 0.81041205, ..., 0.42829758, 0.42465592,\n        0.67533214],\n       [0.11546777, 0.35548417, 0.645703  , ..., 0.24879487, 0.58897384,\n        0.98993676],\n       [0.96847189, 0.21391942, 0.70259718, ..., 0.32546387, 0.97123257,\n        0.99439515]])\n\n\n\n\n\n\n\n\n\n\n\nThe CuPy API is basically Numpy's API, with a few minor differences here and there:\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nB = cp.random.random((N, N))\nB\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[4]:\n\n\n\n\n\narray([[0.5967192 , 0.51631595, 0.49980612, ..., 0.52830527, 0.4521689 ,\n        0.27857874],\n       [0.80999042, 0.32971922, 0.74034167, ..., 0.7316576 , 0.05339145,\n        0.67494372],\n       [0.66954774, 0.08282191, 0.06237442, ..., 0.85821394, 0.33912042,\n        0.00146102],\n       ...,\n       [0.87827673, 0.58662314, 0.97428079, ..., 0.1239315 , 0.90813556,\n        0.55808706],\n       [0.59890383, 0.54480358, 0.59180028, ..., 0.03094922, 0.54241454,\n        0.45274242],\n       [0.34639887, 0.49254118, 0.28915567, ..., 0.86708966, 0.97695957,\n        0.63873008]])\n\n\n\n\n\n\n\n\n\n\n\nOne thing that can be noticed already - that displayed numbers! Right there, in Jupyter! All the memory transfer is done for you as need be, though you can also force it as needed. To me, that's pretty amazing! Let's make sure this is actually on the GPU:\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \n!nvidia-smi\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nTue Jan 22 08:08:36 2019       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 415.27       Driver Version: 415.27       CUDA Version: 10.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 106...  Off  | 00000000:01:00.0 Off |                  N/A |\n| N/A   66C    P2    24W /  N/A |     95MiB /  6078MiB |      1%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0     12081      C   ...ik/.miniconda3/envs/nbody3.6/bin/python    85MiB |\n+-----------------------------------------------------------------------------+\n\n\n\n\n\n\n\n\n\n\n\nClearly a bunch of memory is allocated.\nA few benchmarks\u00b6All right, let's get to the actual number crunching. Let's take the simple element-wise log of each element in the array (on CPU that's going to run with the MKL-accelerated Numpy on an i7):\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \n%%timeit -o\nnp.log(A)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n24.4 ms \u00b1 1.53 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\n\n    Out[6]:\n\n\n\n\n\n<TimeitResult : 24.4 ms \u00b1 1.53 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)>\n\n\n\n\n\n\n\n\n\n\n\nRespectable, I suppose. Let's see how CuPy fares against that:\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \n%%timeit -o\ncp.log(B)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n453 \u00b5s \u00b1 735 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\n\n\n\n\n\n    Out[7]:\n\n\n\n\n\n<TimeitResult : 453 \u00b5s \u00b1 735 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)>\n\n\n\n\n\n\n\n\n\n\n\nInstantly, I noticed two things:\n\nMy laptop fan started spinning up immediately after running that command. Clearly something more intense is going on there.\nMy screen went black. Fun fact: I wrote this post out of bed, without having plugged my laptop in - and my current system configuration did not enjoy having a power-hungry GPU try to run on battery, so it just switched off instantly. Consider yourself warned!\n\nAfter rebooting and using the classic Restart and run all, a third fun fact occured to me: a different SI prefix on the GPU result!\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \n__.average / _.average\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[8]:\n\n\n\n\n\n53.877025466882685\n\n\n\n\n\n\n\n\n\n\n\nThat's a pretty okay speedup for swapping n to c in the import statement.\nLet's see how well it's going to parallelize a matrix multiplication:\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \ncpu_operator = %timeit -o A @ A\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n25.2 ms \u00b1 3.19 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \ngpu_operator = %timeit -o B @ B\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n18.7 ms \u00b1 48.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n\n\n\n\n\n\n\n\n\n\nNote how that's literally the same operation in terms of code, as we're not using Numpy's functions, rather - both of these classes define an @ operator. This is going to come up later...\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \ncpu_operator.average / gpu_operator.average\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[14]:\n\n\n\n\n\n1.3472357440051654\n\n\n\n\n\n\n\n\n\n\n\nWell, suprisingly, this is nowhere near as large of a speedup as I would expect! My results seem to vary a bit, though:\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \ngpu_operator_saved = %timeit -o B2 = B @ B\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n18.7 ms \u00b1 4.83 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \ngpu_dot = %timeit -o cp.dot(B, B)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n9.37 ms \u00b1 10.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \ngpu_dot_saved = %timeit -o B3 = cp.dot(B, B)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n17.4 ms \u00b1 3.26 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[18]:\n\n    \nBtarget= cp.empty_like(B)\ngpu_dot_out = %timeit -o cp.dot(B, B, Btarget)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n18.7 ms \u00b1 9.19 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\n\n\n\nI think I may come back to the matrix multiplication issue in the future, because it seems like there are multiple ways to do it and it's not clear which one is the best. Weirdly, the winner seems to be .dot(B, B), but...without saving. Let's keep this post to an overview of CuPy's functionality and possibly revisit that in the future. This may have been a BLAS/cuBLAS issue that I don't quite understand yet.\nFurther functionality review\u00b6Okay, but what actually is B?\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \ntype(B)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[20]:\n\n\n\n\n\ncupy.core.core.ndarray\n\n\n\n\n\n\n\n\n\n\n\nAll right, some internal Cupy ndarray class. It's pretty simple to turn it into something in host device memory, though:\n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \ntype(cp.asnumpy(B))\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[21]:\n\n\n\n\n\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \nB[0], cp.asnumpy(B)[0]\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[22]:\n\n\n\n\n\n(array([0.5967192 , 0.51631595, 0.49980612, ..., 0.52830527, 0.4521689 ,\n        0.27857874]),\n array([0.5967192 , 0.51631595, 0.49980612, ..., 0.52830527, 0.4521689 ,\n        0.27857874]))\n\n\n\n\n\n\n\n\n\n\n\nJust to make sure this is actually the same array:\n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \nnp.allclose(B, cp.asnumpy(B))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-23-8ff31bdd3597> in <module>\n----> 1 np.allclose(B, cp.asnumpy(B))\n\n~/.local/lib/python3.6/site-packages/numpy/core/numeric.py in allclose(a, b, rtol, atol, equal_nan)\n   2268 \n   2269     \"\"\"\n-> 2270     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n   2271     return bool(res)\n   2272 \n\n~/.local/lib/python3.6/site-packages/numpy/core/numeric.py in isclose(a, b, rtol, atol, equal_nan)\n   2352             return less_equal(abs(x-y), atol + rtol * abs(y))\n   2353 \n-> 2354     x = asanyarray(a)\n   2355     y = asanyarray(b)\n   2356 \n\n~/.local/lib/python3.6/site-packages/numpy/core/numeric.py in asanyarray(a, dtype, order)\n    551 \n    552     \"\"\"\n--> 553     return array(a, dtype, copy=False, order=order, subok=True)\n    554 \n    555 \n\nValueError: object __array__ method not producing an array\n\n\n\n\n\n\n\n\n\n\nClose, but no cigar! I think this may be getting out of date relatively soon, but right now NumPy doesn't know how to handle our B GPU array. Another example of this is:\n\n\n\n\n\n\nIn\u00a0[24]:\n\n    \nnp.log(B)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-24-31256e41f3b1> in <module>\n----> 1 np.log(B)\n\nValueError: object __array__ method not producing an array\n\n\n\n\n\n\n\n\n\n\nWhat you could do instead is compare this right on the GPU, going from GPU to host to GPU again:\n\n\n\n\n\n\nIn\u00a0[25]:\n\n    \ncp.allclose(B, cp.asarray(cp.asnumpy(B)))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-25-159d67e41f21> in <module>\n----> 1 cp.allclose(B, cp.asarray(cp.asnumpy(B)))\n\nAttributeError: module 'cupy' has no attribute 'allclose'\n\n\n\n\n\n\n\n\n\n\nAnd this is, actually, the first time I saw cupy not implementing something in NumPy's API! It's pretty easy to get around this, in this instance:\n\n\n\n\n\n\nIn\u00a0[26]:\n\n    \ncp.all(cp.isclose(B, cp.asarray(cp.asnumpy(B))))\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[26]:\n\n\n\n\n\narray(True)\n\n\n\n\n\n\n\n\n\n\n\nI guess that's no proof that this works correctly, but it's at least an argument :)\nHowever... Wait a minute. What's that array thing doing there? As far as I have been able to figure out, this is a single element array allocated in GPU memory that .all() reduces our boolean NxN isclose array to. It's pretty simple to convert to a normal Python bool, though:\n\n\n\n\n\n\nIn\u00a0[27]:\n\n    \nbool(_)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[27]:\n\n\n\n\n\nTrue\n\n\n\n\n\n\n\n\n\n\n\nReshape works, as does summing along an axis:\n\n\n\n\n\n\nIn\u00a0[28]:\n\n    \nB.reshape(N, N, 1).sum(axis=1)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[28]:\n\n\n\n\n\narray([[504.85090841],\n       [524.895922  ],\n       [511.81485662],\n       ...,\n       [505.07597442],\n       [505.44331639],\n       [508.71877327]])\n\n\n\n\n\n\n\n\n\n\n\nSo do statistical functions:\n\n\n\n\n\n\nIn\u00a0[29]:\n\n    \nB.mean(axis=0)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[29]:\n\n\n\n\n\narray([0.49501721, 0.51289292, 0.51160649, ..., 0.49777249, 0.50287185,\n       0.49873693])\n\n\n\n\n\n\n\n\n\n\n\nYou can raise stuff to powers and sum to scalars:\n\n\n\n\n\n\nIn\u00a0[30]:\n\n    \n(A**3).sum(), (B**3).sum()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[30]:\n\n\n\n\n\n(262231.4456098349, array(262330.20455528))\n\n\n\n\n\n\n\n\n\n\n\nAnd of course, once again we need to force a cast to a Python float:\n\n\n\n\n\n\nIn\u00a0[31]:\n\n    \nfloat(_[1])\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[31]:\n\n\n\n\n\n262330.2045552799\n\n\n\n\n\n\n\n\n\n\n\nYou can also, if you want to, sum into previously allocated arrays (I was thinking of using this to test performance differences between cupy and numba.cuda, haven't gotten to that yet, though):\n\n\n\n\n\n\nIn\u00a0[32]:\n\n    \nAx = np.linspace(0, 1, N)\nAx\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[32]:\n\n\n\n\n\narray([0.00000000e+00, 9.77517107e-04, 1.95503421e-03, ...,\n       9.98044966e-01, 9.99022483e-01, 1.00000000e+00])\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[33]:\n\n    \nA.shape, Ax.shape\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[33]:\n\n\n\n\n\n((1024, 1024), (1024,))\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[34]:\n\n    \nA.sum(axis=1, out=Ax)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[34]:\n\n\n\n\n\narray([497.48857494, 510.25752741, 493.57497492, ..., 515.3009242 ,\n       499.92205554, 512.74308963])\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[35]:\n\n    \nBx = cp.linspace(0, 1, N)\nBx\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[35]:\n\n\n\n\n\narray([0.00000000e+00, 9.77517107e-04, 1.95503421e-03, ...,\n       9.98044966e-01, 9.99022483e-01, 1.00000000e+00])\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[36]:\n\n    \nB.shape, Bx.shape\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[36]:\n\n\n\n\n\n((1024, 1024), (1024,))\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[37]:\n\n    \nB.sum(axis=1, out=Bx)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[37]:\n\n\n\n\n\narray([504.85090841, 524.895922  , 511.81485662, ..., 505.07597442,\n       505.44331639, 508.71877327])\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[38]:\n\n    \nBx\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[38]:\n\n\n\n\n\narray([504.85090841, 524.895922  , 511.81485662, ..., 505.07597442,\n       505.44331639, 508.71877327])\n\n\n\n\n\n\n\n\n\n\n\nRandom numbers start from different seeds:\n\n\n\n\n\n\nIn\u00a0[39]:\n\n    \ncp.random.seed(0)\nRgpu = cp.random.random()\nnp.random.seed(0)\nRcpu = np.random.random()\nRgpu - Rcpu\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[39]:\n\n\n\n\n\narray(0.01273565)\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[40]:\n\n    \ncp.random.seed(0)\nRgpu2 = cp.random.random()\nRgpu2 - Rgpu\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[40]:\n\n\n\n\n\narray(0.)\n\n\n\n\n\n\n\n\n\n\n\nIndexing works just like we know and love it from numpy:\n\n\n\n\n\n\nIn\u00a0[41]:\n\n    \nBx\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[41]:\n\n\n\n\n\narray([504.85090841, 524.895922  , 511.81485662, ..., 505.07597442,\n       505.44331639, 508.71877327])\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[42]:\n\n    \nBx[0] = 3\nBx\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[42]:\n\n\n\n\n\narray([  3.        , 524.895922  , 511.81485662, ..., 505.07597442,\n       505.44331639, 508.71877327])\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[43]:\n\n    \nBx[1::2] = -1\nBx\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[43]:\n\n\n\n\n\narray([  3.        ,  -1.        , 511.81485662, ...,  -1.        ,\n       505.44331639,  -1.        ])\n\n\n\n\n\n\n\n\n\n\n\nThe amazing power tool that is einsum works as well, let's use it to compute the array's trace:\n\n\n\n\n\n\nIn\u00a0[44]:\n\n    \ncp.einsum('ii->', B), cp.sum(cp.diag(B))\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[44]:\n\n\n\n\n\n(array(493.15631992), array(493.15631992))\n\n\n\n\n\n\n\n\n\n\n\nWriting CPU and GPU agnostic code\u00b6This is a concept I found in CuPy's library and absolutely fell in love.\nIn some cases, you can use array methods and operators to do what you need. This is where that A @ A and B @ B concept comes back. However, that's not always possible. For example, there isn't a .log() method.\nHOWEVER, the CuPy folks had a pretty ingenious idea for solving that! Just watch:\n\n\n\n\n\n\nIn\u00a0[45]:\n\n    \ndef agnostic_log(array):\n    xp = cp.get_array_module(array)\n    return xp.log(array)\n\nagnostic_log(A), agnostic_log(B)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[45]:\n\n\n\n\n\n(array([[-2.26442884, -1.28466872, -2.36988397, ..., -0.51318531,\n         -4.16956465, -1.59940023],\n        [-0.20347928, -1.62341712, -0.40219454, ..., -0.17808444,\n         -1.85785034, -0.4825748 ],\n        [-0.30958741, -0.40846499, -0.59633861, ..., -0.37680399,\n         -0.0684416 , -0.16571087],\n        ...,\n        [-0.26502811, -0.91303893, -0.21021246, ..., -0.84793704,\n         -0.85647603, -0.39255065],\n        [-2.15876381, -1.03427455, -0.43741564, ..., -1.39112655,\n         -0.52937351, -0.01011422],\n        [-0.03203582, -1.54215589, -0.35297155, ..., -1.12250382,\n         -0.02918932, -0.00562062]]),\n array([[-0.51630862, -0.66103639, -0.69353501, ..., -0.638081  ,\n         -0.79369951, -1.27805454],\n        [-0.21073286, -1.10951384, -0.30064348, ..., -0.31244264,\n         -2.93010466, -0.39312597],\n        [-0.40115281, -2.4910626 , -2.7746    , ..., -0.15290186,\n         -1.08140002, -6.52862238],\n        ...,\n        [-0.12979355, -0.53337268, -0.02605573, ..., -2.08802627,\n         -0.09636162, -0.5832403 ],\n        [-0.51265425, -0.60732996, -0.52458607, ..., -3.47540747,\n         -0.61172474, -0.79243193],\n        [-1.06016435, -0.70817722, -1.24079009, ..., -0.1426129 ,\n         -0.02331001, -0.44827332]]))\n\n\n\n\n\n\n\n\n\n\n\nSame function handles two completely different array types!\n\n\n\n\n\n\nIn\u00a0[46]:\n\n    \ncp.get_array_module(A), cp.get_array_module(B)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[46]:\n\n\n\n\n\n(<module 'numpy' from '/home/dominik/.local/lib/python3.6/site-packages/numpy/__init__.py'>,\n <module 'cupy' from '/home/dominik/.miniconda3/envs/nbody3.6/lib/python3.6/site-packages/cupy/__init__.py'>)\n\n\n\n\n\n\n\n\n\n\n\nThis is so simple, I absolutely love it. It's not perfect (you still have to define a new function), but it's a nice workaround. It may not be necessary for a lot longer, too...\nAnd given that I'm ending on links, I'll just add Matthew Rocklin's post about prototype GPU arrays on Dask clusters.\nTo sum up: CuPy is awesome, if you've got a GPU lying around (for games, etc), you can very easily use it for your number crunching as well!",
      "tags": "benchmark,cupy,numpy,python",
      "url": "https://stanczakdominik.github.io/posts/numpy-ish-gpu-computations-with-cupy/"
    },
    {
      "title": "About",
      "text": "Hi there! My name's Dominik Sta\u0144czak and I'm currently a student at the University of Warsaw, in Poland.\nMy main passion is physics. Computational physics and plasma physics, two passions,\ncomputational, plasma physics and the open source movement. My three passions are computational and plasma physics,\nthe open source movement, GPU computing, puns and an almost fanatical devotion to sharing knowledge.\nAmongst my passions... let me try this again.\nWhere to find me\nI'm on Twitter.\nI'm on Github.\nI'm on LinkedIn.\nOn Matrix, I'm @StanczakDominik:matrix.org.\nIf you want to contact me, send me dog pics or share a cool plot, my email below or the Matrix address above is probably the best way to reach me.",
      "tags": "",
      "url": "https://stanczakdominik.github.io/about/"
    },
    {
      "title": "An introduction",
      "text": "Why start this blog?\n\nTo share knowledge\nKnowledge does not diminish from being shared, instead it is reinforced and gains in utility.\nI gain much utility from amazing resources on the web and elsewhere. It is only fair that I disseminate some\nof that knowledge. I have lots of projects I start and tinker with. While I try to keep them over at GitHub,\nthey might be more useful to people with actual proper documentation and descriptions.\nBesides, I think I can help some people with stuff like Python, numerics, nonlinear dynamics, quitting vim,\nsetting up and using Linux, fluid and plasma dynamics, GPU computing, machine learning... While my perspective on these may not be helpful to all, I certainly don't expect it\nto do harm.\nBecause some things just make you want to tell the world all about them\nThere's a lot of beautiful science and physics around the world and plenty of it in simulation, which is my personal area of preference. Numerical simulation is the art and science of creating\npretty plots that can also teach us something about the world or about the models we use to try and describe it. but most of us are probably in this for the plots\nLook at this animation of a hydrodynamic\n(Kelvin-Helmholtz\nor Swirly-Vortex) instability simulation I found online, thanks to Kevin\nSchaal and reposted here with permission.\nDo yourself a service and watch it in full HD.\n\nIf that doesn't strike you as beautiful, you may have no emotion. Here's a few reasons why this is awesome to me:\n\nThis kind of pattern occurs in nature. Want an example? Here's one from Biblioteca Pleyades:\n\n  \nYup. That's our very own Sun. And that's the exact same kind of pattern occurring out there.\n\n\nIt's fairly well described on relevant time and spatial scales by a mathematical model people have devised via the art of pondering science.\n\n\nOur computers have the ability to perform this series of computations and spit out a visualization of this kind of resolution. People have gone to the moon on less - and they didn't have all of our current\nfancy tools like high level programming languages, GPUs, autocompletion, documentation, debuggersbar the occasional moth and Stack Overflow.\n\n\nTo sum up... In our modern times, there is absolutely stopping you from sitting down at your computer right now and create good physics  and pretty plots using it.\nBecause writing is fun\nSeriously. If you haven't tried freeflow, stream of consciousness style writing, try it. It's done wonders for me as a way of structuring my thoughts and pointing out\nflaws in my logic. Besides, with the ease of setting up a blog like this one, the cost of sharing your thoughts with the world is marginal.\nTo make some positive impact\nSomebody wise once said,\n\nWhat are the most pressing issues in your field, and why aren't you working on them?\n\nFor the last half a century we've had a painfully underfunded technology capable of transforming energy generation\naround the globe. I am, of course, talking about nuclear fusion. \nWe are currently orbiting a large reactor that's a neat proof of concept for the principle,\nand we're beginning to harness that power using solar cells.\nIt's been amazing to see the growth of that. However, given the rising temperatures of our globe caused by us as a civilization\nrelentlessly pumping carbon dioxides into the atmosphere,\nthis post is brought to you by two open windows and a draught between them\nit doesn't seem that's going to be enough. Not as long as our wires aren't superconducting and we can't just power all the globe using\nAfrican deserts,\n and as long as our best idea for large scale energy storage is pumping water up a hill, \nwe're tied to burning hydrocarbons.\nOr are we?\nThat's the view many people take when faced with the problem, if they even acknowledge man-made global warning at all.\nIt's certainly the view of plenty of politicians all over the world. How do we know that's true?\nNow, I wish I had influence over public policy, but sadly, I don't. But what I can possibly affect, even if just a little bit,\nis public opinion. And here's hoping it propagates outwards.\nWith this blog I am going to try to present an objective view of the state and perspectives of nuclear fusion\nas a possible safe and clean alternative to hydrocarbons. Good people have been working on that for a long time and now\nwe're getting quite close - a few more steps in a long marathon - to getting that actually working.\nBecause faced with a crisis such as climate change, we're going to need all of the tools at our disposal.\nBecause that idea might save the world one day, and that seems like a pretty pressing issue.",
      "tags": "",
      "url": "https://stanczakdominik.github.io/posts/an-introduction/"
    },
    {
      "title": "Search",
      "text": "Search results appear here.",
      "tags": "",
      "url": "https://stanczakdominik.github.io/search/"
    }
  ]
};