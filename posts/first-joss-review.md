<!--
.. title: First JOSS review!
.. slug: first-joss-review
.. date: 2020-04-24 16:44:54 UTC+02:00
.. tags: joss,article
.. category: open-science
.. link: 
.. description: 
.. type: text
-->

Several months ago, I stumbled upon [the journal they call
Joss](https://www.youtube.com/watch?v=Y4eKWN_eAxM). Well, actually, [JOSS - the
Journal of Open Source Software, "a developer friendly, open access journal for
research software packages"](https://joss.theoj.org/). It's a completely free,
open-source and open-access alternative to established,
[for-(often-a-lot-of)-profit](https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science)
journals such as those by Reed-Elsevier or Springer.

And today, [I've been called into
service](https://github.com/openjournals/joss-reviews/issues/2133#issuecomment-618617823)
to review [VlaPy, "1D-1V Vlasov-Poisson(-Fokker-Planck) Plasma Physics
Simulation Tool"](https://github.com/joglekara/VlaPy). I'm really looking forward to it!

<!-- TEASER_END -->

Here are few factoids about JOSS and open science that you might not have been aware of.

## Research software attribution

Acknowledgement and funding for developing and maintaining research software
tends to be sparse. Remember the black hole image from last year? To quote
[Andreas Mueller on
Twitter](https://twitter.com/amuellerml/status/1117455802598662144):

> Slightly ironic that in the same week @NSF rejects a grant to fund the scipy
> ecosystem saying that working on it is not impactful enough and hiring
> developers to work on it is too expensive. 

To counteract that, scientific developers tend to chase exciting new results
that accompany new releases of their software. However, that often leads to
"more software" instead of "better and more stable software".

As for attribution, citations are everything in the current (rather flawed, in
my opinion - I'm not ready with a pull request just yet, though) system of
evaluation of scientific work. Package authors tend to try to write books about
their works that are then cited. This seems to have improved in the recent years,
but I distinctly remember that most `__citation__` atributes for packages in the
Pythonosphere were books a while ago. Getting software that "just works" and 
simplifies your life published, from what I've heard, can be difficult if not
accompanied by a "novel" result.

Meanwhile, [the recent survey of computational tools in solar physics sent out by
the wonderful people working on
SunPy](https://github.com/sunpy/sunpy/wiki/Coordination-Meeting-2020-Notes#survey-of-computational-tools-in-solar-physics)
found that 

> ~75% cited scientific software but only ~40% do it routinely.

Sample size of 364. That's better than I thought it would have been, honestly!

JOSS tries to solve this issue by TODO

## Reproducibility


## Transparency

JOSS reviews happen completely out in the open. No more anonymous reviewer 2
shouting irrelevant abuse at you!

I do wonder a bit about entrenched powerful positions being invulnerable to criticism... TODO

